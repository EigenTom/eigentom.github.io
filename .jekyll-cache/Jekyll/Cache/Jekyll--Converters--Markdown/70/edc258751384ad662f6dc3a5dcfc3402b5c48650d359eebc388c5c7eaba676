I"Bƒ<h1 id="æŸå¤±å‡½æ•°å’Œæ¢¯åº¦ä¸‹é™æ³•">æŸå¤±å‡½æ•°å’Œæ¢¯åº¦ä¸‹é™æ³•</h1>
<h2 id="1-åŸºæœ¬å®šä¹‰å’Œæœ¯è¯­">1. åŸºæœ¬å®šä¹‰å’Œæœ¯è¯­</h2>
<p>æœºå™¨å­¦ä¹ æ˜¯æŒ‡å°½å¯èƒ½é¿å…äººçš„ä»‹å…¥, è€Œä»¤ç¨‹åºå°è¯•ä»æ”¶é›†åˆ°çš„æ•°æ®ä¸­å‘æ˜æ¨¡å¼ä¸è§„å¾‹, ä»è€Œè§£å†³é—®é¢˜çš„æ–¹æ³•. ç¥ç»ç½‘ç»œæˆ–æ·±åº¦å­¦ä¹ ç›¸æ¯”ä»¥å¾€äººä¸ºæŒ‡å®šç‰¹å¾å‘é‡å¹¶ä½¿ç”¨åˆ†ç±»å™¨è¿›è¡Œå­¦ä¹ çš„æ–¹æ³•, æ›´èƒ½é¿å…äººä¸ºä»‹å…¥, é€šè¿‡å¯¹å›¾åƒç›´æ¥å­¦ä¹ ä»è€Œè§£å†³é—®é¢˜. å’Œå¾…å¤„ç†çš„é—®é¢˜æ— å…³, ç¥ç»ç½‘ç»œå¯ä»¥ç›´æ¥å°†æ•°æ®ä½œä¸ºåŸå§‹æ•°æ®è¿›è¡Œ  â€œç«¯åˆ°ç«¯â€ çš„å­¦ä¹ .</p>

<p><strong>å®šä¹‰1.1</strong>: è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®</p>
<blockquote>
  <p><strong>è®­ç»ƒæ•°æ®</strong> æ˜¯ç¥ç»ç½‘ç»œç”¨äºå­¦ä¹ , å¯»æ‰¾æœ€ä¼˜å‚æ•°çš„æ•°æ®. <br />
<strong>æµ‹è¯•æ•°æ®</strong> æ˜¯ç¥ç»ç½‘ç»œå®Œæˆå­¦ä¹ åç”¨äºæ£€éªŒè¯„ä»·è®­ç»ƒæ‰€å¾—çš„æ¨¡å‹çš„å®é™…èƒ½åŠ›çš„æ•°æ®.</p>
</blockquote>

<p>[æ³¨] <br />
ä¸ºäº†æ­£ç¡®è¯„ä»·ç¥ç»ç½‘ç»œè®­ç»ƒæ‰€å¾—æ¨¡å‹çš„ <strong>æ³›åŒ–èƒ½åŠ›</strong> (å³æ‰€å¾—çš„ç¥ç»ç½‘ç»œå¯¹æ–°çš„æ•°æ®æœ‰å¤šå¼ºçš„å¤„ç†èƒ½åŠ›), å°±å¿…é¡»åˆ’åˆ†è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®. ä¸€èˆ¬åœ°, æµ‹è¯•æ•°æ®åˆæˆä¸º <strong>ç›‘ç£æ•°æ®</strong>.</p>

<p><br /></p>

<p><strong>å®šä¹‰1.2</strong>: è¿‡æ‹Ÿåˆ</p>
<blockquote>
  <p>è‹¥è®­ç»ƒå‡ºçš„æ¨¡å‹ä»…ä»…å¯¹æµ‹è¯•æ•°æ®é›†æœ‰å¾ˆå¼ºçš„å¤„ç†èƒ½åŠ›, è€Œå¯¹å…¶ä»–ä¸åŒçš„æ•°æ®é›†å¤„ç†èƒ½åŠ›è¾ƒå·®çš„è¯, åˆ™ç§°åœ¨æ­¤æ¬¡è®­ç»ƒä¸­å‡ºç°äº† <strong>è¿‡æ‹Ÿåˆ</strong> ç°è±¡.</p>
</blockquote>

<p>æœºå™¨å­¦ä¹ çš„æœ€ç»ˆç›®æ ‡æ˜¯: åŸºäºè®­ç»ƒæ•°æ®è·å¾—è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›, å¹¶æœ€å¤§ç¨‹åº¦åœ°é¿å…æ¨¡å‹çš„è¿‡æ‹Ÿåˆ.</p>

<p><br /></p>

<p><strong>å®šä¹‰1.3</strong>: <code class="language-plaintext highlighter-rouge">One-Hot</code> è¡¨ç¤º</p>
<blockquote>
  <p>åœ¨ç¥ç»ç½‘ç»œçš„è¾“å‡ºä¸­, æŠŠæ¯ä¸€ç§è¯†åˆ«ç»“æœè§†ä¸ºä¸€ä¸ªæ ‡ç­¾, å°†æ­£ç¡®çš„è§£æ ‡ç­¾è®¾ä¸º $1$, é”™è¯¯çš„è®¾ä¸º $0$ çš„è¡¨ç¤ºæ–¹æ³•å³ç§°ä¸º <strong><code class="language-plaintext highlighter-rouge">One-Hot</code> è¡¨ç¤º</strong>.</p>
</blockquote>

<p><br /></p>

<p><strong>å®šä¹‰1.4</strong>: æŸå¤±å‡½æ•° (<code class="language-plaintext highlighter-rouge">Loss Function</code>)</p>
<blockquote>
  <p><strong>æŸå¤±å‡½æ•°</strong> æ˜¯è¡¨ç¤ºç¥ç»ç½‘ç»œå¯¹ç›‘ç£æ•°æ® <strong>ä¸æ‹Ÿåˆ</strong> çš„ç¨‹åº¦, æ˜¯è¡¨ç¤ºç¥ç»ç½‘ç»œæ€§èƒ½ <strong>æ¶åŠ£ç¨‹åº¦</strong> çš„æŒ‡æ ‡ åœ¨è®­ç»ƒä¸­, ç¥ç»ç½‘ç»œä»¥æŸå¤±å‡½æ•°ä¸ºçº¿ç´¢å¯»æ‰¾æœ€ä¼˜çš„æƒé‡å‚æ•°.</p>
</blockquote>

<p><br /></p>

<p><strong>å®šä¹‰1.5</strong>: å°æ‰¹é‡å­¦ä¹  (<code class="language-plaintext highlighter-rouge">Mini-Batch Learning</code>)</p>
<blockquote>
  <p>ä»æµ·é‡çš„è®­ç»ƒæ•°æ®é›†ä¸­éšæœºé€‰å‡ºä¸€æ‰¹æ•°æ®, ç„¶åä»¥æ¯ä¸€æ‰¹æ•°æ®ä½œä¸ºè®­ç»ƒæ•°æ®é›†, è¿›è¡Œç¥ç»ç½‘ç»œçš„è®­ç»ƒçš„æ–¹æ³•ç§°ä¸º <strong>å°æ‰¹é‡å­¦ä¹ </strong>.</p>
</blockquote>

<p><br /></p>

<h2 id="2-æŸå¤±å‡½æ•°å’Œå°æ‰¹é‡å­¦ä¹ æ³•">2. æŸå¤±å‡½æ•°å’Œå°æ‰¹é‡å­¦ä¹ æ³•</h2>
<p>æŸå¤±å‡½æ•°ä¸ä»…æ˜¯ä¸€ä¸ªè¡¨ç¤ºç¥ç»ç½‘ç»œæ€§èƒ½çš„æŒ‡æ ‡, å®ƒå¯¹ç¥ç»ç½‘ç»œçš„ â€œè‡ªå­¦ä¹ â€ ä¹Ÿæ˜¯è‡³å…³é‡è¦çš„. åœ¨ç¥ç»ç½‘ç»œçš„å­¦ä¹ è¿‡ç¨‹ä¸­, å¯»æ‰¾æœ€ä¼˜å‚æ•°, ä¹Ÿå°±æ˜¯æƒé‡å’Œåç½®æ—¶, è¦é€‰æ‹©ä¸€ç»„èƒ½å¤Ÿä½¿ç¥ç»ç½‘ç»œçš„æ€§èƒ½ç›¸å¯¹æœ€ä½³çš„å‚æ•°, æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä¼˜åŒ–è¿‡ç¨‹: ä»¥ç¥ç»ç½‘ç»œçš„å‚æ•°ä½œä¸ºå¾…ä¼˜åŒ–å¯¹è±¡, ä¼˜åŒ–ç›®æ ‡æ˜¯ä½¿ç¥ç»ç½‘ç»œçš„æ€§èƒ½ç›¸å¯¹æœ€ä¼˜. è€Œæˆ‘ä»¬çŸ¥é“, åœ¨æ¯ä¸€æ­¥ä¼˜åŒ–ä¸­, å½“å‰çš„è¿™ç»„å¾…ä¼˜åŒ–å‚æ•°éœ€è¦å‘ä¸€ä¸ªç‰¹å®šçš„æ–¹å‘å˜åŒ–, è€Œåœ¨æ¢¯åº¦ä¸‹é™æ³•ä¸­, å¯¹æ¯ä¸ªå‚æ•°è€Œè¨€, å…¶å¯¹åº”çš„å˜åŒ–æ–¹å‘ç”±å’Œå®ƒå¯¹åº”çš„æŸå¤±å‡½æ•°çš„å¯¼æ•°æä¾›.</p>

<p>æˆ‘ä»¬ç«‹åˆ»æ³¨æ„åˆ°, åœ¨è¿™ä¸ªä¼˜åŒ–è¿‡ç¨‹ä¸­, é€‰æ‹©æŸå¤±å‡½æ•°ä½œä¸ºè¯„ä»·å‡½æ•°çš„é‡è¦åŸå› æ˜¯:</p>
<ol>
  <li>æŸå¤±å‡½æ•°éå¸¸çµæ•, å³ä½¿æƒå€¼ä¸­æŸä¸€ä¸ªå‚æ•°å‡ºç°äº†å¾®å°å˜åŒ–ä¹Ÿå¯èƒ½å¯¼è‡´æŸå¤±å‡½æ•°å€¼çš„æ”¹å˜, è¿™å’Œ â€œç¥ç»ç½‘ç»œçš„è¯†åˆ«æ­£ç¡®ç‡â€ è¿™ä¸€å‚æ•°ä¸åŒ.</li>
  <li>æŸå¤±å‡½æ•°çš„å˜åŒ–æ˜¯è¿ç»­çš„, å…¶å¯¼æ•°åœ¨å®šä¹‰åŸŸä¸Šä¸ä¸º $0$, ä»¥å®ƒçš„å¯¼æ•°ä½œä¸ºä¼˜åŒ–æ–¹å‘çš„æŒ‡æ ‡éå¸¸å¯é .</li>
</ol>

<p>ä¸‹é¢æˆ‘ä»¬ä»‹ç»ä¸¤ç§å¸¸è§çš„æŸå¤±å‡½æ•°:</p>

<ol>
  <li>
    <p>å‡æ–¹è¯¯å·® (<code class="language-plaintext highlighter-rouge">Mean Squared Error</code>)<br /></p>

\[E = \frac{1}{2}\sum_{k}(y_k - t_k)^2\]

    <p>å…¶ä¸­, $y_k$ è¡¨ç¤ºç¥ç»ç½‘ç»œçš„è¾“å‡º, $t_k$ è¡¨ç¤ºç›‘ç£æ•°æ® (æµ‹è¯•æ•°æ®), $k$ æ˜¯æ•°æ®çš„ç»´åº¦.</p>

    <p>å‡æ–¹è¯¯å·®çš„ <code class="language-plaintext highlighter-rouge">Python</code> å®ç°å¦‚ä¸‹:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre> <span class="k">def</span> <span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
     <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
    <p><br /></p>
  </li>
  <li>
    <p>äº¤å‰ç†µè¯¯å·® (<code class="language-plaintext highlighter-rouge">Cross Entropy Error</code>) <br /></p>

\[E = -\sum_{k}t_k\ln{(y_k)}\]

    <p>å…¶ä¸­, $y_k$ ä¸ºç¥ç»ç½‘ç»œçš„è¾“å‡º, $t_k$ ä¸ºæ­£ç¡®è§£æ ‡ç­¾, ä¸”è§£æ ‡ç­¾ä¸­, åªæœ‰æ­£ç¡®çš„å€¼ä¸º $1$, å…¶ä½™çš„å‡ä¸º $0$.</p>

    <p>å€¼å¾—æ³¨æ„çš„æ˜¯, åœ¨äº¤å‰ç†µè¯¯å·®å…¬å¼ä¸­, å®é™…ä¸Šåªä¼šè®¡ç®—æ­£ç¡®è§£æ ‡ç­¾çš„è¾“å‡ºçš„è‡ªç„¶å¯¹æ•°. ä¹Ÿå°±æ˜¯è¯´, äº¤å‰ç†µè¯¯å·®æ˜¯ç”±æ­£ç¡®è§£æ ‡ç­¾æ‰€å¯¹åº”çš„è¾“å‡ºç»“æœå†³å®šçš„.</p>

    <p>äº¤å‰ç†µè¯¯å·®çš„ <code class="language-plaintext highlighter-rouge">Python</code> å®ç°å¦‚ä¸‹:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre> <span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
     <span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-7</span>
     <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">delta</span><span class="p">))</span>   <span class="c1"># prevent np.log(0) happen
</span></pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
</ol>

<p>æˆ‘ä»¬å·²ç»çŸ¥é“å°æ‰¹é‡å­¦ä¹ çš„å®šä¹‰, é€šè¿‡ä½¿ç”¨ <code class="language-plaintext highlighter-rouge">numpy</code> åº“å†…å»ºçš„ <code class="language-plaintext highlighter-rouge">random.choice()</code> å‡½æ•°å°±å¯ä»¥å®ç°å®ƒ:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">train_size</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
<span class="n">t_batch</span> <span class="o">=</span> <span class="n">t_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<h2 id="3-æ•°å€¼å¾®åˆ†å’Œæ¢¯åº¦ä¸‹é™æ³•">3. æ•°å€¼å¾®åˆ†å’Œæ¢¯åº¦ä¸‹é™æ³•</h2>

<p><strong>æ•°å€¼å¾®åˆ†</strong> æ˜¯ä¸€ç§ç”¨å‡½æ•°çš„å€¼å’Œå…¶ä»–å·²çŸ¥ä¿¡æ¯æ¨ç®—è¯¥å‡½æ•°å¯¼æ•°çš„è®¡ç®—æ–¹æ³•.</p>

<p>åœ¨æ¢¯åº¦ä¸‹é™æ³•ä¸­, æˆ‘ä»¬ä½¿ç”¨ä¸¤ç‚¹ä¼°è®¡æ³•è®¡ç®—å‡½æ•°çš„ä¸€é˜¶å‡å·® $\frac{f(x + \Delta x) - f(x)}{\Delta x}$, å¹¶ä»¥å…¶è¿‘ä¼¼è§†ä¸ºå‡½æ•°åœ¨ç‚¹ $x$ å¤„çš„å¯¼æ•°.</p>

<p>ç”±å¤šå…ƒå‡½æ•° $f(x_1, x_2, \cdots, x_n)$ æ‰€æœ‰çš„åå¯¼æ•° $\frac{\partial(f)}{\partial(x_i)}, ~~~ i \in [n]$ æ„æˆçš„å‘é‡ $(\frac{\partial(f)}{\partial(x_1)}, \frac{\partial(f)}{\partial(x_2)} \cdots, \frac{\partial(f)}{\partial(x_n)})$ ç§°ä¸ºè¿™ä¸ªå¤šå…ƒå‡½æ•° $f$ çš„ <strong>æ¢¯åº¦</strong> (<code class="language-plaintext highlighter-rouge">gradient</code>).</p>

<p>å¯¹ç»™å®šå‡½æ•°æ¢¯åº¦çš„è®¡ç®—å¯ä»¥å¦‚ä¸‹å®ç°:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span> <span class="c1"># 0.0001
</span>    
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">):</span>
        <span class="n">tmp_val</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="o">+</span> <span class="n">h</span>
        <span class="n">fxh1</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># f(x+h)
</span>        
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="o">-</span> <span class="n">h</span> 
        <span class="n">fxh2</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># f(x-h)
</span>
        <span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh1</span> <span class="o">-</span> <span class="n">fxh2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="c1"># è¿˜åŸå€¼
</span>        
    <span class="k">return</span> <span class="n">grad</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>å‡½æ•° $f(x_1, x_2) = x_1^2 + x_2^2$ çš„æ¢¯åº¦å¦‚ä¸‹å›¾æ‰€ç¤º:</p>

<p><img src="https://cdn.jsdelivr.net/gh/KirisameMarisaa/KirisameMarisaa.github.io/img/blogpost_images/gradient-2d.jpg" alt="gradient-2d" /></p>

<p>å¯è§:</p>
<ol>
  <li>æ¢¯åº¦æ‰€æŒ‡ç¤ºçš„æ–¹å‘æ˜¯å‡½æ•°å€¼å‡å°æœ€å¤šçš„æ–¹å‘</li>
  <li>è·ç¦»å±€éƒ¨æœ€ä¼˜ç‚¹æœ€è¿œ, æ¢¯åº¦å€¼è¶Šå¤§. (ä½“ç°åœ¨ç®­å¤´çš„é•¿åº¦ä¸Š)</li>
</ol>

<p>æ¢¯åº¦ä¸‹é™æ³•æ˜¯ä¸€ç§ä¼˜åŒ–æ–¹æ³•. åœ¨æ¯ä¸€æ¬¡ä¼˜åŒ–è¿‡ç¨‹ä¸­, åŸºäºå‡½æ•°çš„å½“å‰å–å€¼, æ²¿å…¶æ¢¯åº¦æ–¹å‘å‰è¿›ç»™å®šçš„ä¸€æ®µè·ç¦», å¹¶åœ¨ä¸‹ä¸€æ¬¡ä¼˜åŒ–è¿‡ç¨‹é‡Œåœ¨æ–°ä½ç½®å¤„é‡æ–°è®¡ç®—æ¢¯åº¦, æ²¿æ¢¯åº¦æ–¹å‘ç»§ç»­å‰è¿›. é€šè¿‡ä¸æ–­åœ°æ²¿æ¢¯åº¦æ–¹å‘å‰è¿›, ä¼šé€æ¸æ‰¾åˆ°ä½¿å‡½æ•°å€¼ <strong>ç›¸å¯¹æœ€å°</strong> çš„å‚æ•°. (æ€è€ƒ: ä¸ºä»€ä¹ˆæ˜¯ â€œç›¸å¯¹æœ€å°â€? ä»€ä¹ˆæ˜¯ â€œå±€éƒ¨æœ€ä¼˜â€? å¦‚ä½•ç ´è§£é™·å…¥å±€éƒ¨æœ€ä¼˜çš„æƒ…å†µ?)</p>

<p>åœ¨æ¢¯åº¦ä¸‹é™æ³•ä¸­, æ¯ä¸€æ­¥ä¼˜åŒ–è¿‡ç¨‹é‡Œå‰è¿›çš„è·ç¦» $\eta$ åœ¨ç¥ç»ç½‘ç»œçš„å­¦ä¹ ä¸­ç§°ä¸º <strong>å­¦ä¹ ç‡</strong>. æ¢¯åº¦ä¸‹é™æ³•çš„åŸºæœ¬å®ç°å¦‚ä¸‹:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">step_num</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    
    <span class="c1"># Args:
</span>    <span class="c1">#   f: function to be learned (optimized)
</span>    <span class="c1">#   init_x: initial value
</span>    <span class="c1">#   lr: learning rate
</span>    <span class="c1">#   step_num: the number of iterations (learn how many times)
</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">init_x</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_num</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>ç¥ç»ç½‘ç»œçš„æ¢¯åº¦æ˜¯æŒ‡ç¥ç»ç½‘ç»œä¸­æŸå¤±å‡½æ•°å…³äºæƒé‡å‚æ•°çš„æ¢¯åº¦:</p>

<p>[ä¾‹]<br />
ç¥ç»ç½‘ç»œ <code class="language-plaintext highlighter-rouge">SimpleNet</code> çš„æƒé‡ $W$ å¤§å°ä¸º $2 \cdot 3$, æŸå¤±å‡½æ•°ä¸º $L$. åˆ™å…¶æ¢¯åº¦ $\frac{\partial(L)}{\partial(W)}$ ä¸º:</p>

\[\frac{\partial(L)}{\partial(W)} = \begin{pmatrix} \frac{\partial(L)}{\partial(w_{11})}, \frac{\partial(L)}{\partial(w_{12})}, \frac{\partial(L)}{\partial(w_{13})}  \\ ~~~ \\\frac{\partial(L)}{\partial(w_{21})}, \frac{\partial(L)}{\partial(w_{22})}, \frac{\partial(L)}{\partial(w_{23})}\end{pmatrix}\]

<p>å…¶ä¸­
\(W = \begin{pmatrix} w_{11}, w_{12}, w_{13} \\ ~~ \\ w_{21}, w_{22}, w_{23}\end{pmatrix}\)</p>

<p>å¯¹å…¶æ±‚æ¢¯åº¦çš„ <code class="language-plaintext highlighter-rouge">Python</code> å®ç°å¦‚ä¸‹:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">simpleNet</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<h2 id="4-éšæœºæ¢¯åº¦ä¸‹é™æ³•çš„å®ç°">4. éšæœºæ¢¯åº¦ä¸‹é™æ³•çš„å®ç°</h2>

<p>å…ˆåšä¸€ä¸ªç®€å•çš„æ€»ç»“: ç¥ç»ç½‘ç»œçš„éšæœºæ¢¯åº¦ä¸‹é™æ³•å­¦ä¹ æ­¥éª¤åŸºæœ¬å¦‚ä¸‹:</p>

<ol>
  <li>ä»è®­ç»ƒæ•°æ®ä¸­éšæœºæŠ½å‡ºä¸€éƒ¨åˆ†, ç§°å…¶ä¸º <code class="language-plaintext highlighter-rouge">mini-batch</code>. å­¦ä¹ çš„ç›®æ ‡æ˜¯ä¼˜åŒ–å‚æ•°, å°½å¯èƒ½åœ°ä½¿ <code class="language-plaintext highlighter-rouge">mini-batch</code> çš„æŸå¤±å‡½æ•°æœ€å°åŒ–.</li>
  <li>æ±‚å‡ºå„ä¸ªæƒé‡å‚æ•°çš„æ¢¯åº¦</li>
  <li>å°†æƒé‡å‚æ•°æ²¿æ¢¯åº¦æ–¹å‘è¿›è¡Œæ›´æ–°</li>
  <li>é‡å¤ä¸Šè¿°æ­¥éª¤, ç›´åˆ°é‡å¤æ¬¡æ•°è¾¾åˆ°è®¾å®šå€¼.</li>
</ol>

<p>ä¸‹é¢, æˆ‘ä»¬å®ç°ä¸€ä¸ªåŠŸèƒ½ä¸ºè¯†åˆ«æ‰‹å†™æ•°å­—, ä½¿ç”¨ <code class="language-plaintext highlighter-rouge">MNIST</code> æ•°æ®é›†è¿›è¡Œå­¦ä¹ çš„ $2$ å±‚ç¥ç»ç½‘ç»œ <code class="language-plaintext highlighter-rouge">Two Layer Net</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">TwoLayerNet</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">weight_init_std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="c1"># åˆå§‹åŒ–æƒé‡
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span>
        <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span>
    
        <span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
        <span class="n">z1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
        <span class="n">a2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">y</span>
        
    <span class="c1"># x:è¾“å…¥æ•°æ®, t:ç›‘ç£æ•°æ®
</span>
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">accuracy</span>
        
    <span class="c1"># x:è¾“å…¥æ•°æ®, t:ç›‘ç£æ•°æ®
</span>
    <span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">loss_W</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        
        <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">])</span>
        
        <span class="k">return</span> <span class="n">grads</span>
        
    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span>
        <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
        
        <span class="n">batch_num</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># forward
</span>        
        <span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
        <span class="n">z1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
        <span class="n">a2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
        
        <span class="c1"># backward
</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_num</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z1</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">da1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">W2</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">dz1</span> <span class="o">=</span> <span class="n">sigmoid_grad</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span> <span class="o">*</span> <span class="n">da1</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dz1</span><span class="p">)</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dz1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grads</span>
</pre></td></tr></tbody></table></code></pre></div></div>

:ET