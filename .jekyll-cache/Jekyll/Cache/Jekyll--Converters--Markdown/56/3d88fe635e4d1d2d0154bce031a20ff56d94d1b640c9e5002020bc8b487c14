I"l°<h1 id="è¯¯å·®åå‘ä¼ æ’­æ³•">è¯¯å·®åå‘ä¼ æ’­æ³•</h1>
<h2 id="1-åå‘ä¼ æ’­">1. åå‘ä¼ æ’­</h2>
<p>åœ¨å‰ä¸€ç« æ‰€ä»‹ç»çš„å‰é¦ˆç¥ç»ç½‘ç»œä¸­,ç¥ç»ç½‘ç»œçš„è¾“å…¥å±‚, ç»è¿‡å„ä¸ªéšè—å±‚æ¿€æ´»å‡½æ•°çš„è®¡ç®—ååˆ°è¾¾è¾“å‡ºå±‚, å¹¶è¾“å‡ºç»“æœ. åœ¨æ•´ä¸ªè®¡ç®—è¿‡ç¨‹ä¸­, æ•°æ®å§‹ç»ˆéµå¾ª â€œä»å‰ä¸€å±‚æµåŠ¨åˆ°å½“å‰å±‚, åœ¨å½“å‰å±‚è¢«å¤„ç†åå†æµå‘ä¸‹ä¸€å±‚â€ çš„æµåŠ¨æ–¹å‘, ä¹Ÿå°±æ˜¯æ­£å‘æµåŠ¨. å› æ­¤, è¿™æ ·çš„è¿‡ç¨‹ä¹Ÿè¢«ç§°ä¸º <strong>æ­£å‘ä¼ æ’­</strong>.</p>

<p>ä¸‹é¢æˆ‘ä»¬è€ƒè™‘å’Œæ­£å‘ä¼ æ’­ç›¸å¯¹çš„ <strong>åå‘ä¼ æ’­</strong>: ç”±äºåœ¨ä¸åŒèŠ‚ç‚¹å¤„æ•°æ®éƒ½è¢«ç»è¿‡ç›¸è¾ƒäºæ•´ä½“è€Œè¨€ç®€å•ä¸€äº›çš„å•ç‹¬å¤„ç†, å› æ­¤æˆ‘ä»¬å¯ä»¥åŸºäºå¯¼å‡½æ•°è®¡ç®—çš„é“¾å¼æ³•åˆ™, åˆ†åˆ«åœ¨æ¯ä¸ªèŠ‚ç‚¹å¤„åŸºäºæ•°æ®çš„åå‘æµåŠ¨æ–¹å‘, è¯¥èŠ‚ç‚¹çš„æ¿€æ´»å‡½æ•°, ä»¥åŠæ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­, åœ¨è¯¥èŠ‚ç‚¹çš„è¾“å…¥/è¾“å‡º, è®¡ç®—å‡ºè¯¥èŠ‚ç‚¹çš„æ¢¯åº¦, ä»è€Œè®¡ç®—å‡ºæŸå¤±å‡½æ•°å€¼å¯¹æ¯ä¸€ä¸ªå‚é‡çš„æ¢¯åº¦.</p>

<p>ä½¿ç”¨åå‘ä¼ æ’­è®¡ç®—æŸå¤±å€¼å…³äºå‚é‡çš„åå¯¼æ•°ç›¸æ¯”ä¸€é˜¶å‡å·®æ³•è¦å¿«å¾—å¤š, å¹¶ä¸”å¯¹äºå…·æœ‰å¤šä¸ªéšè—å±‚çš„å¤æ‚ç¥ç»ç½‘ç»œ, åå‘ä¼ æ’­è®¡ç®—æ³•è¿˜å¯ä»¥è®¡ç®—å‡ºæŸå¤±å‡½æ•°å€¼å…³äºéšè—å±‚ä¸­å‚é‡å’Œæƒå€¼çš„åå¯¼æ•° (æ•°æ®åå‘æµåŠ¨åˆ°è¯¥éšè—å±‚å°±åœæ­¢, æ­¤æ—¶çš„è®¡ç®—ç»“æœå°±æ˜¯æˆ‘ä»¬æ‰€éœ€è¦çš„).</p>

<p>åå‘ä¼ æ’­çš„åŸºæœ¬è¿‡ç¨‹æ˜¯:</p>
<ol>
  <li>æ­£å‘ä¼ æ’­æ—¶, åœ¨è®¡ç®—å›¾ä¸­, æ¯ä¸€ä¸ªè®¡ç®—èŠ‚ç‚¹ä¿ç•™å…¶è¾“å…¥å€¼å’Œè¾“å‡ºå€¼, å¹¶è®¡ç®—å‡º<strong>è¯¥èŠ‚ç‚¹è¾“å‡ºå€¼å…³äºè¾“å…¥å€¼</strong>çš„åå¯¼æ•°, ä»¥ä¾›åœ¨åå‘ä¼ æ’­æ—¶ä½¿ç”¨è¯¥èŠ‚ç‚¹çš„å±€éƒ¨æ¢¯åº¦.</li>
  <li>å®Œæˆæ­£å‘ä¼ æ’­, åŸºäºè¾“å‡ºå€¼å’Œæ ‡ç­¾, ä»£å…¥åˆ°æŸå¤±å‡½æ•°ä¸­è®¡ç®—å¾—åˆ°æŸå¤±å€¼.</li>
  <li>è¿›è¡Œåå‘ä¼ æ’­: ä»è¾“å‡ºèŠ‚ç‚¹å¼€å§‹åå‘æº¯æº, å¯¹æ¯ä¸€æ¡æ•°æ®æµåŠ¨è·¯å¾„è€Œè¨€, éƒ½è¦å°†å…¶é‡åˆ°çš„æ‰€æœ‰èŠ‚ç‚¹å¤„çš„å±€éƒ¨æ¢¯åº¦ç›¸ä¹˜, ç›´åˆ°åå‘æµåŠ¨è‡³è¾“å…¥å±‚, æ­¤æ—¶å…¶æ€»æ¢¯åº¦å°±æ˜¯è¯¥å‚é‡å¯¹åº”çš„, æŸå¤±å‡½æ•°å€¼å¯¹è¯¥å‚é‡çš„æ¢¯åº¦.</li>
</ol>

<p><br /></p>

<h2 id="2-ä¹˜æ³•å±‚å’ŒåŠ æ³•å±‚çš„å®ç°">2. ä¹˜æ³•å±‚å’ŒåŠ æ³•å±‚çš„å®ç°</h2>

<p>åå‘ä¼ æ’­åŸºäºé“¾å¼æ³•åˆ™æˆç«‹. ä¸‹é¢æˆ‘ä»¬ä»‹ç»åå‘ä¼ æ’­ä¸­çš„ä¹˜æ³•å±‚å’ŒåŠ æ³•å±‚, ä»¥æ­¤æ·±åŒ–å¯¹åå‘ä¼ æ’­ç»“æ„çš„ç†è§£.</p>

<p>æˆ‘ä»¬é¦–å…ˆè€ƒè™‘åŠ æ³•èŠ‚ç‚¹çš„åå‘ä¼ æ’­. <br />
ä»¥ $z = x + y$ ä¸ºå¯¹è±¡, è§‚å¯Ÿå¯çŸ¥å…¶å…³äº $x, y$ çš„åå¯¼æ•°å‡ä¸º $1$. è¿™è¯´æ˜, åœ¨åŠ æ³•èŠ‚ç‚¹çš„åå‘ä¼ æ’­ä¸­, è¾“å…¥å€¼ä¼šåŸå°ä¸åŠ¨åœ°ç›´æ¥ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹.</p>

<p>åŠ æ³•å±‚çš„ <code class="language-plaintext highlighter-rouge">Python</code> å®ç°å¦‚ä¸‹:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">AddLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="mi">1</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<p>æˆ‘ä»¬ç»§ç»­è§‚å¯Ÿä¹˜æ³•å±‚. è€ƒè™‘ $f = x\cdot y\cdot z$, å¯çŸ¥:
\(\frac{\partial f}{\partial x} = yz, ~~~ \frac{\partial f}{\partial y} = xz, ~~~ \frac{\partial f}{\partial z} = xy.\)</p>

<p>è¿™è¯´æ˜, åœ¨ä¹˜æ³•èŠ‚ç‚¹çš„åå‘ä¼ æ’­ä¸­, è¾“å…¥å€¼ä¼šè¢«ä¹˜ä»¥æ­£å‘ä¼ æ’­æ—¶, é™¤å»åå‘ä¼ æ’­çš„ä¸‹æ¸¸æ–¹å‘æ‰€å¯¹åº”çš„è¾“å…¥ä¿¡å·ä»¥å¤–, å…¶ä½™æ‰€æœ‰è¾“å…¥ä¿¡å·çš„ä¹˜ç§¯.</p>

<p>ä¹˜æ³•å±‚çš„ <code class="language-plaintext highlighter-rouge">Python</code> å®ç°å¦‚ä¸‹:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MulLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>                
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">x</span>

        <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<h2 id="3-æ¿€æ´»å‡½æ•°å±‚çš„å®ç°">3. æ¿€æ´»å‡½æ•°å±‚çš„å®ç°</h2>
<p>ä¸‹é¢, æˆ‘ä»¬å°†åå‘ä¼ æ’­æ€æƒ³å’Œç¥ç»ç½‘ç»œç»“åˆ, å°†æ„æˆç¥ç»ç½‘ç»œçš„å±‚å®ç°ä¸ºç±», å®ç° <code class="language-plaintext highlighter-rouge">ReLU</code> å‡½æ•°å±‚å’Œ <code class="language-plaintext highlighter-rouge">Sigmoid</code> å‡½æ•°å±‚:</p>

<p><code class="language-plaintext highlighter-rouge">ReLU</code> å‡½æ•° $r(x)$:</p>

\[r(X) = \begin{cases} x ~~~ (x &gt; 0) \\ 0 ~~~ (x \leqslant 0)\end{cases}\]

<p>å¯çŸ¥å…¶å¯¼å‡½æ•°ä¸º:</p>

\[\frac{\partial r}{\partial x} = \begin{cases} 1 ~~~ (x &gt; 0) \\ 0 ~~~ (x \leqslant 0)\end{cases}\]

<p>å¯çŸ¥, å¯¹äº <code class="language-plaintext highlighter-rouge">ReLU</code> å‡½æ•°å±‚, è‹¥æ­£å‘ä¼ æ’­æ—¶çš„è¾“å…¥ $x$ å¤§äº $0$, åˆ™åå‘ä¼ æ’­æ—¶, ä¼šå°†ä¸Šæ¸¸çš„å€¼åŸå°ä¸åŠ¨åœ°ä¼ é€’ç»™ä¸‹æ¸¸, åä¹‹æ¥è‡ªä¸Šæ¸¸çš„å€¼å°†ä¼šåœåœ¨æ­¤å¤„, å¦‚åŒä¸€ä¸ªå…·æœ‰è®°å¿†åŠŸèƒ½çš„ç”µè·¯å¼€å…³.</p>

<p><code class="language-plaintext highlighter-rouge">ReLU</code> å‡½æ•°å±‚çš„ <code class="language-plaintext highlighter-rouge">Python</code> å®ç°å¦‚ä¸‹: (ä¸ºç¡®ä¿æ³›ç”¨æ€§, æ­¤å¤„æˆ‘ä»¬å°†æ­£å‘ä¼ æ’­æ—¶çš„è¾“å…¥è¾“å‡ºå‡è§†ä¸º <code class="language-plaintext highlighter-rouge">numpy</code> æ•°ç»„)</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Relu</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">out</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dout</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span>

        <span class="k">return</span> <span class="n">dx</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>[æ³¨]<br />
<code class="language-plaintext highlighter-rouge">self.mask</code> æ˜¯ä¸€ä¸ªç”± <code class="language-plaintext highlighter-rouge">True</code>, <code class="language-plaintext highlighter-rouge">False</code>æ„æˆçš„, å’Œ $x$ å°ºå¯¸ä¸€è‡´çš„æ•°ç»„, å®ƒä½œä¸º $x$ çš„ â€œé®ç½©â€, å°†æ­£å‘ä¼ æ’­æ—¶, $x$ çš„å…ƒç´ ä¸­å…¨éƒ¨å°äºç­‰äº $0$ çš„å…ƒç´ ä¿å­˜ä¸º $<code class="language-plaintext highlighter-rouge">True</code>$, å…¶ä½™çš„ä¿å­˜ä¸º <code class="language-plaintext highlighter-rouge">False</code>. åœ¨åå‘è¾“å‡ºæ—¶, è‹¥ <code class="language-plaintext highlighter-rouge">self.mask</code> ä¸­å’Œ  $x$ å¯¹åº”ä½ç½®çš„å…ƒç´ ä¸º <code class="language-plaintext highlighter-rouge">True</code>, åˆ™ä½œä¸ºè¾“å‡ºçš„åŒå°ºå¯¸æ•°ç»„ <code class="language-plaintext highlighter-rouge">dout</code> çš„å¯¹åº”ä½ä¸º $0$, åä¹‹åˆ™ä¸º $x$ çš„å¯¹åº”å€¼.</p>

<p><br /></p>

<p><code class="language-plaintext highlighter-rouge">Sigmoid</code> å‡½æ•° $S(x)$:</p>

\[S(X) = \frac{1}{1 + \exp^{(-x)}}.\]

<p><code class="language-plaintext highlighter-rouge">Sigmoid</code> å‡½æ•°çš„è®¡ç®—å¯åŸºäºå››åˆ™è¿ç®—çš„ä¼˜å…ˆçº§é¡ºåºè§†ä¸ºä¸€ä¸ªç”±å¤šä¸ªèŠ‚ç‚¹è®¡ç®—æ‰€æ„æˆçš„è®¡ç®—. å…¶è®¡ç®—å›¾å¦‚ä¸‹:</p>

<p><img src="https://cdn.jsdelivr.net/gh/KirisameMarisaa/KirisameMarisaa.github.io/img/blogpost_images/20210113100439.png" alt="20210113100439" /></p>

<p>ç”±è®¡ç®—å›¾ä¸­çš„æ¨å¯¼å¯çŸ¥, <code class="language-plaintext highlighter-rouge">Sigmoid</code> å‡½æ•°å±‚çš„åå‘ä¼ æ’­åå¯¼å‡½æ•°ä¸º:</p>

\[\frac{\partial S}{\partial x} = S(1-S).\]

<p><code class="language-plaintext highlighter-rouge">Sigmoid</code> å‡½æ•°å±‚çš„ <code class="language-plaintext highlighter-rouge">Python</code> å®ç°å¦‚ä¸‹:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">out</span>

        <span class="k">return</span> <span class="n">dx</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<h2 id="4-affinesoftmax-å±‚çš„å®ç°">4. <code class="language-plaintext highlighter-rouge">Affine/SoftMax</code> å±‚çš„å®ç°</h2>
<p>ä¸‹é¢æˆ‘ä»¬åˆ†åˆ«å®ç° <code class="language-plaintext highlighter-rouge">Affine</code> å±‚å’Œ <code class="language-plaintext highlighter-rouge">SoftMax</code> å±‚.
åœ¨å‡ ä½•æ„ä¹‰ä¸Š, ä»¿å°„å˜æ¢ (<code class="language-plaintext highlighter-rouge">Affine Transformation</code>) åŒ…æ‹¬ä¸€æ¬¡çº¿æ€§å˜æ¢å’Œä¸€æ¬¡å¹³ç§», å¯¹åº”åˆ°ç¥ç»ç½‘ç»œä¸­å°±æ˜¯ä¸€æ¬¡åŠ æƒå’Œè¿ç®—ä¸ä¸€æ¬¡åŠ åç½®è¿ç®—:
\(Y = X \cdot W + B.\)</p>

<p>åœ¨ç¥ç»ç½‘ç»œçš„ä»¿å°„å˜æ¢ä¸­, å„ä¸ªèŠ‚ç‚¹æ‰€è¿›è¡Œçš„è¿ç®—éƒ½æ˜¯çŸ©é˜µè¿ç®—. åŸºäºçŸ©é˜µçš„æ±‚å¯¼æ³•åˆ™, æˆ‘ä»¬å¯å¾—:</p>

\[\frac{\partial Y}{\partial X} = W^\mathrm{T}, ~~~ \frac{\partial{Y}}{\partial W} = X^\mathrm{T}.\]

<p>æ‰¹ç‰ˆæœ¬çš„ <code class="language-plaintext highlighter-rouge">Affine</code> å±‚è®¡ç®—å›¾å¦‚ä¸‹å›¾æ‰€ç¤º:</p>

<p><img src="https://cdn.jsdelivr.net/gh/KirisameMarisaa/KirisameMarisaa.github.io/img/blogpost_images/20210113102429.png" alt="20210113102429" /></p>

<p>å…¶ <code class="language-plaintext highlighter-rouge">Python</code> å®ç°å¦‚ä¸‹:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Affine</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span><span class="n">W</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dW</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">db</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">dx</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<p>æˆ‘ä»¬æœ€åå¯¹ <code class="language-plaintext highlighter-rouge">SoftMax</code> å‡½æ•°è¿›è¡Œç®€è¦ä»‹ç». åœ¨ç¬¬ä¸€ç« ä¸­æˆ‘ä»¬å·²ç»äº†è§£, <code class="language-plaintext highlighter-rouge">SoftMax</code> å‡½æ•°ä¼šå°†è¾“å…¥å€¼æ­£è§„åŒ–åå†è¾“å‡º. è€ƒè™‘åˆ° <code class="language-plaintext highlighter-rouge">SoftMax</code> å±‚åŒæ ·åŒ…å«ä½œä¸ºæŸå¤±å‡½æ•°çš„äº¤å‰ç†µè¯¯å·®, å› æ­¤åˆå°†å…¶ç§°ä¸º <code class="language-plaintext highlighter-rouge">SoftMax-with-Loss</code> å±‚. å…¶è®¡ç®—å›¾å¦‚ä¸‹æ‰€ç¤º:</p>

<p><img src="https://cdn.jsdelivr.net/gh/KirisameMarisaa/KirisameMarisaa.github.io/img/blogpost_images/20210113105156.png" alt="20210113105156" /></p>

<p>å›¾ä¸­å°† <code class="language-plaintext highlighter-rouge">softmax</code> å‡½æ•°è®°ä¸º <code class="language-plaintext highlighter-rouge">SoftMax</code> å±‚, äº¤å‰ç†µè¯¯å·®è®°ä¸º <code class="language-plaintext highlighter-rouge">Cross Entropy Error</code> å±‚, å¹¶ä¸”å‡è®¾è¿›è¡Œä¸‰ç±»åˆ†ç±».</p>

<p>å…¶ <code class="language-plaintext highlighter-rouge">Python</code> å®ç°å¦‚ä¸‹:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">SoftmaxWithLoss</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># softmaxçš„è¾“å‡º
</span>        
        <span class="bp">self</span><span class="p">.</span><span class="n">t</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># ç›‘ç£æ•°æ®
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t</span> <span class="o">=</span> <span class="n">t</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_error</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">loss</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">.</span><span class="n">size</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">:</span> <span class="c1"># ç›‘ç£æ•°æ®æ˜¯one-hot-vectorçš„æƒ…å†µ
</span>
            <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">dx</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">t</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="n">dx</span> <span class="o">=</span> <span class="n">dx</span> <span class="o">/</span> <span class="n">batch_size</span>
        
        <span class="k">return</span> <span class="n">dx</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<h2 id="5-è¯¯å·®åå‘ä¼ æ’­æ³•çš„å®ç°">5. è¯¯å·®åå‘ä¼ æ’­æ³•çš„å®ç°</h2>
<p>å…ˆåšä¸€ä¸ªç®€å•çš„æ€»ç»“. ç¥ç»ç½‘ç»œå­¦ä¹ çš„å…¨è¿‡ç¨‹å¤§è‡´ä¸º;</p>
<ol>
  <li>ä»è®­ç»ƒæ•°æ®ä¸­éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†æ•°æ®è¿›è¡Œå°æ‰¹é‡å­¦ä¹ .</li>
  <li>è®¡ç®—æŸå¤±å‡½æ•°å…³äºå„ä¸ªæƒé‡å‚æ•°çš„æ¢¯åº¦, è¯¯å·®åå‘ä¼ æ’­æ³•ä¼šåœ¨è¯¥æ­¥éª¤å†…å‡ºç°.</li>
  <li>å°†æƒé‡å‚æ•°å…³äºå…¶å¯¹åº”æ¢¯åº¦æ–¹å‘è¿›è¡Œå¾®å°çš„æ›´æ–°.</li>
  <li>å¯¹å‰ä¸‰ä¸ªæ­¥éª¤è¿›è¡Œæœ‰é™æ¬¡é‡å¤.</li>
</ol>

<p>å¯¹åº”è¯¯å·®åå‘ä¼ æ’­æ³•çš„ç¥ç»ç½‘ç»œçš„å®ç°å¦‚ä¸‹:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">TwoLayerNet</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">weight_init_std</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">):</span>
        <span class="c1"># åˆå§‹åŒ–æƒé‡
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span> 
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>

        <span class="c1"># ç”Ÿæˆå±‚
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">Affine</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Relu1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">Relu</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">Affine</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">])</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">lastLayer</span> <span class="o">=</span> <span class="n">SoftmaxWithLoss</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>
        
    <span class="c1"># x:è¾“å…¥æ•°æ®, t:ç›‘ç£æ•°æ®
</span>
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">lastLayer</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">t</span><span class="p">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">accuracy</span>
        
    <span class="c1"># x:è¾“å…¥æ•°æ®, t:ç›‘ç£æ•°æ®
</span>
    <span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">loss_W</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        
        <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">])</span>
        
        <span class="k">return</span> <span class="n">grads</span>
        
    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="c1"># forward
</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

        <span class="c1"># backward
</span>
        <span class="n">dout</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">dout</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lastLayer</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>
        
        <span class="n">layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">values</span><span class="p">())</span>
        <span class="n">layers</span><span class="p">.</span><span class="n">reverse</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="n">dout</span> <span class="o">=</span> <span class="n">layer</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>

        <span class="c1"># è®¾å®š
</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine1'</span><span class="p">].</span><span class="n">dW</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine1'</span><span class="p">].</span><span class="n">db</span>
        <span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine2'</span><span class="p">].</span><span class="n">dW</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="s">'Affine2'</span><span class="p">].</span><span class="n">db</span>

        <span class="k">return</span> <span class="n">grads</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>æ³¨æ„: åœ¨è¿™é‡Œ, æˆ‘ä»¬å°†ç¥ç»ç½‘ç»œçš„å±‚å£°æ˜ä¸ºæœ‰åºå­—å…¸ <code class="language-plaintext highlighter-rouge">OrderedDict</code>. è¿™ä½¿å¾—ç¥ç»ç½‘ç»œçš„æ­£å‘/åå‘ä¼ æ’­åªéœ€æŒ‰ç…§é¡ºåºè°ƒç”¨å„å±‚çš„ <code class="language-plaintext highlighter-rouge">forward()</code> æˆ– <code class="language-plaintext highlighter-rouge">backward</code> æ–¹æ³•å³å¯.</p>

<p>ä½¿ç”¨è¯¯å·®åå‘ä¼ æ’­æ³•çš„å­¦ä¹ å®ç°å¦‚ä¸‹:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
</pre></td><td class="rouge-code"><pre><span class="c1"># è¯»å…¥æ•°æ®
</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">load_mnist</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">one_hot_label</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">iters_num</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">train_loss_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_acc_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_acc_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">iter_per_epoch</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">train_size</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters_num</span><span class="p">):</span>
    <span class="n">batch_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
    <span class="n">t_batch</span> <span class="o">=</span> <span class="n">t_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
    
    <span class="c1"># æ¢¯åº¦
</span>
    <span class="c1">#grad = network.numerical_gradient(x_batch, t_batch)
</span>    <span class="c1"># æ­¤å¤„ä½¿ç”¨è¯¯å·®åå‘ä¼ æ’­æ³•è®¡ç®—æ¢¯åº¦
</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>
    
    <span class="c1"># æ›´æ–°
</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">(</span><span class="s">'W1'</span><span class="p">,</span> <span class="s">'b1'</span><span class="p">,</span> <span class="s">'W2'</span><span class="p">,</span> <span class="s">'b2'</span><span class="p">):</span>
        <span class="n">network</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
    
    <span class="n">loss</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>
    <span class="n">train_loss_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">iter_per_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">train_acc</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>
        <span class="n">train_acc_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
        <span class="n">test_acc_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">train_acc</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">)</span>

</pre></td></tr></tbody></table></code></pre></div></div>
:ET