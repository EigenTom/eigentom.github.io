<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
    <meta name="description" content="由于相关法律法规规定, 该内容无法显示.">
    <meta name="keywords" content="菜">
    <meta name="theme-color" content="#333">

    <!-- Open Graph -->
    <meta property="og:title"
        content="从零开始的深度学习 Ch1 - 某一般线性空间 | Ramdom Linear Space">
    
    <meta property="og:type" content="article">
    <meta property="og:description" content="感知机与神经网络
1. 感知机的定义
动物的神经系统中最基础的组成单元为神经元 (神经细胞), 用于接受刺激, 产生兴奋并传导兴奋. 神经元有且只有激活态和非激活态两种状态, 并且只有神经元处于激活态时, 传入的兴奋才会由它传出.
">
    
    <meta property="article:published_time" content=" 2021-01-08T00:00:00Z">
    
    
    <meta property="article:author" content="R1NG">
    
    
    <meta property="article:tag" content="扩展自习">
    
    <meta property="article:tag" content="2021">
    
    <meta property="article:tag" content="机器学习">
    
    
    <meta property="og:image" content="http://localhost:4000https://github.com/KirisameR.png">
    <meta property="og:url" content="http://localhost:4000/2021/01/08/Ch1-DeepLearningNotes/">
    <meta property="og:site_name" content="某一般线性空间 | Ramdom Linear Space">

    <title>从零开始的深度学习 Ch1 - 某一般线性空间 | Ramdom Linear Space</title>

    <!-- Web App Manifest -->
    <link rel="manifest" href="/pwa/manifest.json">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/favicon.ico">

    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:4000/2021/01/08/Ch1-DeepLearningNotes/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href=" /css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href=" /css/hux-blog.min.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet"type="text/css">


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>

    <!-- 数学公式 -->
    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
            showProcessingMessages: true, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            extensions: ["tex2jax.js"],
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a'], //避开某些标签
                ignoreClass:"comment-content" //避开含该Class的标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX","TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
    </script>

    

    <!-- Google AdSense -->
    <script data-ad-client="ca-pub-6487568398225121" async
        src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->

    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="/">RANDOM LINEAR SPACE</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div id="huxblog_navbar">
                <div class="navbar-collapse">
                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="/">Home</a>
                        </li>
                        
                        
                        
                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                        
                        
                        <li>
                            <a href="/archive/">Archive</a>
                        </li>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <li class="search-icon">
                            <a href="javascript:void(0)">
                                <i class="fa fa-search"></i>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <script>
        // Drop Bootstarp low-performance Navbar
        // Use customize navbar with high-quality material design animation
        // in high-perf jank-free CSS3 implementation
        var $body = document.body;
        var $toggle = document.querySelector('.navbar-toggle');
        var $navbar = document.querySelector('#huxblog_navbar');
        var $collapse = document.querySelector('.navbar-collapse');

        var __HuxNav__ = {
            close: function () {
                $navbar.className = " ";
                // wait until animation end.
                setTimeout(function () {
                    // prevent frequently toggle
                    if ($navbar.className.indexOf('in') < 0) {
                        $collapse.style.height = "0px"
                    }
                }, 400)
            },
            open: function () {
                $collapse.style.height = "auto"
                $navbar.className += " in";
            }
        }

        // Bind Event
        $toggle.addEventListener('click', function (e) {
            if ($navbar.className.indexOf('in') > 0) {
                __HuxNav__.close()
            } else {
                __HuxNav__.open()
            }
        })

        /**
         * Since Fastclick is used to delegate 'touchstart' globally
         * to hack 300ms delay in iOS by performing a fake 'click',
         * Using 'e.stopPropagation' to stop 'touchstart' event from 
         * $toggle/$collapse will break global delegation.
         * 
         * Instead, we use a 'e.target' filter to prevent handler
         * added to document close HuxNav.  
         *
         * Also, we use 'click' instead of 'touchstart' as compromise
         */
        document.addEventListener('click', function (e) {
            if (e.target == $toggle) return;
            if (e.target.className == 'icon-bar') return;
            __HuxNav__.close();
        })
    </script>
    <!-- Search -->
<div class="search-page">
  <div class="search-icon-close-container">
    <span class="search-icon-close">
      <i class="fa fa-times"></i>
    </span>
  </div>
  <div class="search-main container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <form></form>
        <input type="text" id="search-input" placeholder="$ grep...">
        </form>
        <div id="search-results" class="mini-post-list"></div>
      </div>
    </div>
  </div>
</div>

    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/post-bg-deeplearning.jpg" width="0" height="0"> -->

<!-- Post Header -->



<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/post-bg-deeplearning.jpg');
        background: ;
    }

    
</style>

<header class="intro-header" >

    <div class="header-mask"></div>
    
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/archive/?tag=%E6%89%A9%E5%B1%95%E8%87%AA%E4%B9%A0" title="扩展自习">扩展自习</a>
                        
                        <a class="tag" href="/archive/?tag=2021" title="2021">2021</a>
                        
                        <a class="tag" href="/archive/?tag=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" title="机器学习">机器学习</a>
                        
                    </div>
                    <h1>从零开始的深度学习 Ch1</h1>
                    
                    <h2 class="subheading">Perceptron & Neural Network</h2>
                    <span class="meta">Posted by R1NG on January 8, 2021</span>
                </div>
            </div>
        </div>
    </div>
</header>






<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <!-- Multi-Lingual -->
                

				<h1 id="感知机与神经网络">感知机与神经网络</h1>
<h2 id="1-感知机的定义">1. 感知机的定义</h2>
<p>动物的神经系统中最基础的组成单元为神经元 (神经细胞), 用于接受刺激, 产生兴奋并传导兴奋. 神经元有且只有激活态和非激活态两种状态, 并且只有神经元处于激活态时, 传入的兴奋才会由它传出.</p>

<p>感知机是神经网络最基本的组成单元, 其本质是以特征向量为自变量的分段函数, 能够完整地模拟神经元的逻辑功能.</p>

<p>一个标准的感知机包含以下三个组成部分:</p>
<ol>
  <li>输入 (<code class="language-plaintext highlighter-rouge">Input</code>)</li>
  <li>与每一个输入所对应的权值 (<code class="language-plaintext highlighter-rouge">Weights</code>)</li>
  <li>激活函数 (<code class="language-plaintext highlighter-rouge">Function</code>)</li>
</ol>

<p><img src="https://cdn.jsdelivr.net/gh/KirisameMarisaa/KirisameMarisaa.github.io@master/img/blogpost_images/perceptron.jpg" alt="perceptron" /></p>

<p>我们定义:</p>

\[X = (1, x_1, \cdots, x_n),~~ W = (w_0, w_1, \cdots, w_n).\]

<p>其中, $X$ 为激活函数的输入, $W$ 为权值. 则激活函数形为:</p>

\[f(X) = \begin{cases}
      0 ~~~~~~ X \cdot W^T \leqslant \theta \\ 
      1 ~~~~~~ X \cdot W^T &gt; \theta
\end{cases}\]

<p>其中, $\theta$ 称为激活函数 $f$ 的 <strong>阈值</strong>. 函数的分段条件可被转为 $X \cdot W^{T} + b ~~~(b = -\theta)$. 称 $b$ 为 <strong>偏置</strong>, 控制该感知机被激活的难易程度, $W$ 为 <strong>权重</strong>. 控制各个变量的重要程度.</p>

<p>不难看出, 激活函数 $f$ 是一个线性分段函数, 该函数的行为可看作对一个二维平面使用一条直线进行分割. 激活函数的线性性质决定了它无法对二维平面进行较复杂 (非线性) 的分割, 这一问题在我们使用感知机处理某些分类问题时会立即凸显, 比如使用感知机实现异或逻辑门. 首先, 我们来看几个简单的例子:</p>

<p>[例] 使用感知机实现与, 或, 与非门:</p>

<p>与, 或, 与非门的真值表如下:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">$x_1$</th>
      <th style="text-align: center">$x_2$</th>
      <th style="text-align: center">AND</th>
      <th style="text-align: center">OR</th>
      <th style="text-align: center">NAND</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$1$</td>
      <td style="text-align: center">$1$</td>
      <td style="text-align: center">$1$</td>
      <td style="text-align: center">$1$</td>
      <td style="text-align: center">$0$</td>
    </tr>
    <tr>
      <td style="text-align: center">$1$</td>
      <td style="text-align: center">$0$</td>
      <td style="text-align: center">$0$</td>
      <td style="text-align: center">$1$</td>
      <td style="text-align: center">$1$</td>
    </tr>
    <tr>
      <td style="text-align: center">$0$</td>
      <td style="text-align: center">$1$</td>
      <td style="text-align: center">$0$</td>
      <td style="text-align: center">$1$</td>
      <td style="text-align: center">$1$</td>
    </tr>
    <tr>
      <td style="text-align: center">$0$</td>
      <td style="text-align: center">$0$</td>
      <td style="text-align: center">$0$</td>
      <td style="text-align: center">$0$</td>
      <td style="text-align: center">$1$</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>


<span class="k">def</span> <span class="nf">AND</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.7</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">w</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">tmp</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">OR</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.2</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">w</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">tmp</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">NAND</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">])</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mf">0.7</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">w</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">tmp</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>将与门, 或门, 与非门的两个输入 $x_1, x_2$ 视为平面直角坐标系的两轴, 将平面上的点视为所定义的逻辑门函数的两个输入, 可以看出: 对于上述的三种逻辑门电路而言, 其输入-输出分别将平面划分为了两个部分, 且这样的划分是线性的.</p>

<p><br /></p>

<h2 id="2-感知机的局限性和扩展的多层感知机-以异或门电路的实现为例">2. 感知机的局限性和扩展的多层感知机: 以异或门电路的实现为例</h2>
<p>下面, 我们以异或门电路的分析和实现说明感知机 (单层感知机) 的局限性.</p>

<p>异或逻辑门的真值表如下:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">$x_1$</th>
      <th style="text-align: center">$x_2$</th>
      <th style="text-align: center">XOR</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$1$</td>
      <td style="text-align: center">$1$</td>
      <td style="text-align: center">$0$</td>
    </tr>
    <tr>
      <td style="text-align: center">$1$</td>
      <td style="text-align: center">$0$</td>
      <td style="text-align: center">$1$</td>
    </tr>
    <tr>
      <td style="text-align: center">$0$</td>
      <td style="text-align: center">$1$</td>
      <td style="text-align: center">$1$</td>
    </tr>
    <tr>
      <td style="text-align: center">$0$</td>
      <td style="text-align: center">$0$</td>
      <td style="text-align: center">$0$</td>
    </tr>
  </tbody>
</table>

<p>实际上, 我们并不能使用单层感知机实现异或逻辑门. 不妨这样思考: 假定我们使用单层感知机实现了一个异或逻辑门函数 <code class="language-plaintext highlighter-rouge">XOR(x_1, x_2)</code>, 则它在几何意义上必定是一个使用线性函数对二维平面的二分. 而在平面上描点观察可知, 我们并不能使用线性函数对其基于函数输出的不同而进行分割 (即将所有的红色点和蓝色点分隔开). 因此由矛盾推出, 单层感知机无法实现异或逻辑门:</p>

<p><img src="https://cdn.jsdelivr.net/gh/KirisameMarisaa/KirisameMarisaa.github.io@master/img/blogpost_images/xor-gate.jpg" alt="xor-gate" /></p>

<p>不过, 我们可以使用多层感知机的叠加层实现对该平面的分割. 简单推导逻辑表达式可知:</p>

<center>

$\mathbf{XOR}(x_1, x_2) = \mathbf{AND} (\mathbf{NAND}(x_1, x_2), \mathbf{OR}(x_1, x_2))$

</center>

<p>使用之前定义的与门, 或门和与非门就可以这样实现异或门:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">XOR</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">s1</span> <span class="o">=</span> <span class="n">NAND</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">s2</span> <span class="o">=</span> <span class="n">OR</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">AND</span><span class="p">(</span><span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>形如异或门这样, 叠加了多层的感知机称为 <strong>多层感知机</strong> (<code class="language-plaintext highlighter-rouge">Multi-layered Perceptron</code>). 在我们的例子中, 异或门由两级含有权重的层和一级输出层组成. 通过叠加层, 感知机可以进行更为灵活的数据分类和表示.</p>

<p><br /></p>

<h2 id="3-神经网络的定义和层级结构">3. 神经网络的定义和层级结构</h2>
<p>感知机可以通过多层叠加表示复杂函数, 适应多种分类问题. 但感知机的权重需要人为设定, 而且不同的权重会影响感知机的分类表现. 神经网络的作用是自动地从数据中 “学习” 并更新到合适的权重参数.</p>

<p>如下图所示, 神经网络一般有三种分层: 输入层, 中间层 (隐藏层) 和输出层. 神经网络的结构和多层感知机所组成的网络没有区别, 但其本质差异是: 神经网络采用连续函数而非线性分段函数作为激活函数.</p>

<p><img src="https://cdn.jsdelivr.net/gh/KirisameMarisaa/KirisameMarisaa.github.io/img/blogpost_images/neural-network-diagram.jpg" alt="neural-network-diagram" /></p>

<p><br /></p>

<h2 id="4-激活函数的定义和常见的激活函数">4. 激活函数的定义和常见的激活函数</h2>
<p>在介绍感知机时我们已经了解, 激活函数接收所有的输入信号, 并将输入信号的总和转换为输出信号, 而其核心作用在于决定如何激活输入信号的总和. 
下面, 我们介绍数个常用的神经网络激活函数:</p>

<ol>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Sigmoid</code> 函数<br />
<code class="language-plaintext highlighter-rouge">Sigmoid</code> 函数 $S(x)$:</p>

\[S(X) = \frac{1}{1 + \exp^{(-x)}}.\]

    <p>在 <code class="language-plaintext highlighter-rouge">Python</code> 中, <code class="language-plaintext highlighter-rouge">Sigmoid</code> 函数实现如下:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

 <span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
     <span class="k">return</span> <span class="mi">1</span><span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>

    <p><br /></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">ReLU</code> 函数<br />
 <code class="language-plaintext highlighter-rouge">ReLU</code> 函数 $r(x)$:</p>

\[r(X) = \begin{cases} x ~~~ (x &gt; 0) \\ 0 ~~~ (x \leqslant 0)\end{cases}\]

    <p>在 <code class="language-plaintext highlighter-rouge">Python</code> 中, <code class="language-plaintext highlighter-rouge">ReLU</code> 函数实现如下:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

 <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
     <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
</ol>

<p><br /></p>

<ol>
  <li>
    <p><code class="language-plaintext highlighter-rouge">softmax</code> 函数<br />
<code class="language-plaintext highlighter-rouge">softmax</code> 函数 $s(x)$:</p>

\[S(x) = \frac{\exp(a_k)}{\sum_{1}^{n}exp(a_i)}\]

    <p>需要注意的是, <code class="language-plaintext highlighter-rouge">softmax</code> 函数的实现中涉及指数函数计算, 而在指数函数值过大时可能会溢出为 <code class="language-plaintext highlighter-rouge">inf</code>. 若分子和分母均溢出的话, 就无法正常地进行除法运算.</p>

    <p>要解决这一问题, 我们对 <code class="language-plaintext highlighter-rouge">softmax</code> 函数作如下修正:</p>

    <p><br /></p>

\[m = \max(a_1, a_2, \cdots, a_n)\]

\[S_1(x) = \frac{\exp(a_k - m)}{\sum_{1}^{n}exp(a_i - m)}\]

    <p><br /></p>

    <p>这样, 就在不改变运算的结果 (思考一下: 为什么?) 的情况下, 实现了函数的修正. 合理的 <code class="language-plaintext highlighter-rouge">Python</code> 实现如下:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

 <span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
     <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
     <span class="n">exp_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">c</span><span class="p">)</span>
     <span class="n">sum_exp_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_a</span><span class="p">)</span>
     <span class="n">y</span> <span class="o">=</span> <span class="n">exp_a</span> <span class="o">/</span> <span class="n">sum_exp_a</span>

     <span class="k">return</span> <span class="n">y</span>
</pre></td></tr></tbody></table></code></pre></div>    </div>
    <p><code class="language-plaintext highlighter-rouge">softmax</code> 函数的一个有趣的特性是, 对任何输入值, 其函数值均在 $0, 1$ 之间, 且输出总和为 $1$. 基于这个性质, 我们可以将函数的输出解读为概率, 并用概率的工具和方法处理问题.</p>
  </li>
</ol>

<p><br /></p>

<p>神经网络在解决不同类型问题, 如分类问题或回归问题 (预测问题)上时, 需要基于问题类型相应地选择输出层的激活函数. 一般地, 回归问题要用恒等函数, 而分类问题使用 <code class="language-plaintext highlighter-rouge">softmax</code> 函数.</p>

<p><br /></p>

<h2 id="5-三层神经网络的实现">5. 三层神经网络的实现</h2>

<p>下面我们实现连接结构如下图所示的 $3$ 层神经网络:</p>

<p><img src="https://cdn.jsdelivr.net/gh/KirisameMarisaa/KirisameMarisaa.github.io/img/blogpost_images/3-layers-neural-network.jpg" alt="3-layers-neural-network" /></p>

<p>为了分辨复杂的层间连接, 我们引入下图所示的记号方法:</p>

<p><img src="https://cdn.jsdelivr.net/gh/KirisameMarisaa/KirisameMarisaa.github.io/img/blogpost_images/3-layers-neural-network-notations.jpg" alt="3-layers-neural-network-notations" /></p>

<p>在本例中, 我们在全部两层中间层处均使用 <code class="language-plaintext highlighter-rouge">softmax</code> 函数作为激活函数, 在输出层采用恒等函数作为激活函数. 并且为了更好地表示偏置, 我们在每一层都添加了一个用于表示偏置的 <strong>偏置神经元</strong>, 其输入恒为 $1$, 且不和任何其他层连接.</p>

<p>基于以上的连接结构和记号,  对于中间层的第一层, 我们有:</p>

\[a_{1}^{(1)} = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1\]

<p>推广到全部的三个加权和, 有:</p>

<p><br /></p>

\[A^{(1)} = (a_{1}^{(1)}, a_{2}^{(1)}, a_{3}^{(1)}), ~~~ X = (x_1, x_2), ~~~ B^{(1)} = (b_{1}^{(1)}, b_{2}^{(1)}, b_{1}^{(3)})\]

\[W^{(1)} = \begin{pmatrix} w^{(1)}_{11}, w^{(1)}_{21}, w^{(1)}_{31} \\ ~ \\ w^{(1)}_{12}, w^{(1)}_{22}, w^{(1)}_{32} \end{pmatrix}\]

\[A^{(1)} = XW^{(1)} + B^{(1)}\]

<p><br /></p>

<p>而被激活函数转换后所得的信号 $Z_1 = \mathbf{Sigmoid}(A^{(1)}).$</p>

<p>我们可以将该实现方式进一步推广至全部层, 这样就实现了三层神经网络的设计. 其 <code class="language-plaintext highlighter-rouge">Python</code> 实现如下:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">functions</span> <span class="kn">import</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">identity_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">init_network</span><span class="p">():</span>
    <span class="n">network</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">network</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]])</span>
    <span class="n">network</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
    <span class="n">network</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]])</span>
    <span class="n">network</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
    <span class="n">network</span><span class="p">[</span><span class="s">'W3'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span> <span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]])</span>
    <span class="n">network</span><span class="p">[</span><span class="s">'b3'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">network</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">W3</span> <span class="o">=</span> <span class="n">network</span><span class="p">[</span><span class="s">'W1'</span><span class="p">],</span> <span class="n">network</span><span class="p">[</span><span class="s">'W2'</span><span class="p">],</span> <span class="n">network</span><span class="p">[</span><span class="s">'W3'</span><span class="p">]</span>
    <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="n">network</span><span class="p">[</span><span class="s">'b1'</span><span class="p">],</span> <span class="n">network</span><span class="p">[</span><span class="s">'b2'</span><span class="p">],</span> <span class="n">network</span><span class="p">[</span><span class="s">'b3'</span><span class="p">]</span>

    <span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">z1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
    <span class="n">a2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">z2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
    <span class="n">a3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z2</span><span class="p">,</span> <span class="n">W3</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">identity_function</span><span class="p">(</span><span class="n">a3</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">y</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">init_network</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>    <span class="c1">#the output should be [0.31682708 0.69627909]
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p><br /></p>

<h2 id="6-神经网络的输出">6. 神经网络的输出</h2>

<p>我们使用 <code class="language-plaintext highlighter-rouge">MNIST</code> 手写数字图像数据集, 以一个识别手写数字的三层神经网络为例简介神经网络的输出.</p>

<p><code class="language-plaintext highlighter-rouge">MNIST</code> 的图像数据为 $28$px * $28$px 的灰度图像. 依照图片所包含的像素数量和我们需要识别的数字种类, 确定神经网络的输入层有 $784$ 个神经元, 输出层有 $10$ 个神经元. 其隐藏层又由 $50$ 个神经元构成的第一隐藏层和 $100$ 个神经元构成的第二隐藏层组成. 在原书提供的源代码中, 提供了现成的 <code class="language-plaintext highlighter-rouge">MNIST</code> 数据集抓取和转换函数, 而神经网络的权值保存在 <code class="language-plaintext highlighter-rouge">sample_weight.pkl</code> 这个 <code class="language-plaintext highlighter-rouge">Pickel</code> 文件中, 在定义神经网络时被直接读取.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">sys</span><span class="p">,</span> <span class="n">os</span>
<span class="n">sys</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">pardir</span><span class="p">)</span>  <span class="c1"># 为了导入父目录的文件而进行的设定
</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">dataset.mnist</span> <span class="kn">import</span> <span class="n">load_mnist</span>
<span class="kn">from</span> <span class="nn">common.functions</span> <span class="kn">import</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">softmax</span>


<span class="k">def</span> <span class="nf">get_data</span><span class="p">():</span>
    <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">load_mnist</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">flatten</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">one_hot_label</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span>


<span class="k">def</span> <span class="nf">init_network</span><span class="p">():</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"sample_weight.pkl"</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">network</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">network</span>


<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">W3</span> <span class="o">=</span> <span class="n">network</span><span class="p">[</span><span class="s">'W1'</span><span class="p">],</span> <span class="n">network</span><span class="p">[</span><span class="s">'W2'</span><span class="p">],</span> <span class="n">network</span><span class="p">[</span><span class="s">'W3'</span><span class="p">]</span>
    <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="n">network</span><span class="p">[</span><span class="s">'b1'</span><span class="p">],</span> <span class="n">network</span><span class="p">[</span><span class="s">'b2'</span><span class="p">],</span> <span class="n">network</span><span class="p">[</span><span class="s">'b3'</span><span class="p">]</span>

    <span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">z1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
    <span class="n">a2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">z2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
    <span class="n">a3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z2</span><span class="p">,</span> <span class="n">W3</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">a3</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">y</span>


<span class="n">x</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">()</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">init_network</span><span class="p">()</span>
<span class="n">accuracy_cnt</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">p</span><span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># 获取概率最高的元素的索引
</span>    
    <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="n">t</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
        <span class="n">accuracy_cnt</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy:"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">accuracy_cnt</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>在执行上述代码后, 可见 <code class="language-plaintext highlighter-rouge">Console</code> 输出:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">Accuracy</span><span class="p">:</span><span class="mf">0.9352</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>可见这个处理 <code class="language-plaintext highlighter-rouge">MNIST</code> 数据集的神经网络已经成功运行, 并具有 $93.52\%$ 的识别精度.</p>


                <hr style="visibility: hidden;">
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2021/01/07/Ch0-DeepLearningNotes/" data-toggle="tooltip" data-placement="top" title="从零开始的深度学习 Ch0">
                        Previous<br>
                        <span>从零开始的深度学习 Ch0</span>
                        </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2021/01/08/Ch2-DeepLearningNotes/" data-toggle="tooltip" data-placement="top" title="从零开始的深度学习 Ch2">
                        Next<br>
                        <span>从零开始的深度学习 Ch2</span>
                        </a>
                    </li>
                    
                </ul>
                <hr style="visibility: hidden;">

                
                <!-- disqus 评论框 start -->
                <div class="comment">
                    <div id="disqus_thread" class="disqus-thread"></div>
                </div>
                <!-- disqus 评论框 end -->
                

                
            </div>  

    <!-- Side Catalog Container -->
        
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                


<section>
    
        <hr class="hidden-sm hidden-xs">
    
    <h5><a href="/archive/">FEATURED TAGS</a></h5>
    <div class="tags">
        
        
        
        
        
        
                <a data-sort="0109" 
                    href="/archive/?tag=COMP12111"
                    title="COMP12111"
                    rel="10">COMP12111</a>
        
                <a data-sort="0027" 
                    href="/archive/?tag=2021"
                    title="2021"
                    rel="92">2021</a>
        
                <a data-sort="0073" 
                    href="/archive/?tag=%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0"
                    title="课程笔记"
                    rel="46">课程笔记</a>
        
                <a data-sort="0079" 
                    href="/archive/?tag=%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0"
                    title="前端学习"
                    rel="40">前端学习</a>
        
                <a data-sort="0079" 
                    href="/archive/?tag=50P50D"
                    title="50P50D"
                    rel="40">50P50D</a>
        
                <a data-sort="0094" 
                    href="/archive/?tag=2020"
                    title="2020"
                    rel="25">2020</a>
        
                <a data-sort="0104" 
                    href="/archive/?tag=COMP24011"
                    title="COMP24011"
                    rel="15">COMP24011</a>
        
                <a data-sort="0107" 
                    href="/archive/?tag=%E9%80%BB%E8%BE%91%E5%AD%A6"
                    title="逻辑学"
                    rel="12">逻辑学</a>
        
                <a data-sort="0109" 
                    href="/archive/?tag=COMP15111"
                    title="COMP15111"
                    rel="10">COMP15111</a>
        
                <a data-sort="0110" 
                    href="/archive/?tag=%E6%89%A9%E5%B1%95%E8%87%AA%E4%B9%A0"
                    title="扩展自习"
                    rel="9">扩展自习</a>
        
                <a data-sort="0110" 
                    href="/archive/?tag=COMP21111"
                    title="COMP21111"
                    rel="9">COMP21111</a>
        
                <a data-sort="0112" 
                    href="/archive/?tag=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"
                    title="机器学习"
                    rel="7">机器学习</a>
        
                <a data-sort="0112" 
                    href="/archive/?tag=COMP11212"
                    title="COMP11212"
                    rel="7">COMP11212</a>
        
                <a data-sort="0115" 
                    href="/archive/?tag=%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95"
                    title="数据结构和算法"
                    rel="4">数据结构和算法</a>
        
                <a data-sort="0115" 
                    href="/archive/?tag=COMP15212"
                    title="COMP15212"
                    rel="4">COMP15212</a>
        
                <a data-sort="0115" 
                    href="/archive/?tag=COMP23111"
                    title="COMP23111"
                    rel="4">COMP23111</a>
        
                <a data-sort="0116" 
                    href="/archive/?tag=%E5%A5%87%E6%8A%80%E6%B7%AB%E5%B7%A7"
                    title="奇技淫巧"
                    rel="3">奇技淫巧</a>
        
                <a data-sort="0117" 
                    href="/archive/?tag=%E7%99%BD%E6%9E%9C%E5%9B%AD"
                    title="白果园"
                    rel="2">白果园</a>
        
                <a data-sort="0117" 
                    href="/archive/?tag=%E7%AE%97%E6%B3%95"
                    title="算法"
                    rel="2">算法</a>
        
                <a data-sort="0117" 
                    href="/archive/?tag=Lab"
                    title="Lab"
                    rel="2">Lab</a>
    </div>
</section>


                <!-- Friends Blog -->
                
<hr>
<h5>FRIENDS</h5>
<ul class="list-inline">
  
  <li><a href="http://ryanxin.cn">Ryan Xin</a></li>
  
  <li><a href="https://flyhigher.top">Axton.Gay</a></li>
  
  <li><a href="https://graynekocafe.net">GrayNekoBean</a></li>
  
  <li><a href="Gnefil.github.io">Gnefil</a></li>
  
</ul>

            </div>
        </div>
    </div>
</article>

<!-- add support for mathjax by voleking-->






<!-- disqus 公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "KirisameR";
    var disqus_identifier = "/2021/01/08/Ch1 DeepLearningNotes";
    var disqus_url = "http://localhost:4000/2021/01/08/Ch1-DeepLearningNotes/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<!-- disqus 公共JS代码 end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'right',
          // icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <!-- SNS Link -->
                


<ul class="list-inline text-center">


  
  
  
  
  
  
  <li>
    <a target="_blank" href="https://github.com/KirisameR">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
  
  <li>
    <a target="_blank" href="https://www.linkedin.com/in/yilu-82589933">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
</ul>

                <p class="copyright text-muted">
                    Copyright &copy; 某一般线性空间 2022
                    <br>
                    Powered by <a href="http://huangxuan.me">Hux Blog</a> |
                    <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="100px"
                        height="20px"
                        src="https://ghbtns.com/github-btn.html?user=huxpro&repo=huxpro.github.io&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<!-- Currently, only navbar scroll-down effect at desktop still depends on this -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>

<!-- Simple Jekyll Search -->
<script src="/js/simple-jekyll-search.min.js"></script>

<!-- Service Worker -->

<script src="/js/snackbar.js "></script>
<script src="/js/sw-registration.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
        var d = document, t = 'script',
            o = d.createElement(t),
            s = d.getElementsByTagName(t)[0];
        o.src = u;
        if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
        s.parentNode.insertBefore(o, s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,
     From V1.6, There is no need for Highlight.js,
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
     - https://github.com/jneen/rouge/wiki/list-of-supported-languages-and-lexers
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->







<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function () {
        var $nav = document.querySelector("nav");
        if ($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->



<!-- Baidu Tongji -->



<!-- Side Catalog -->

<script type="text/javascript">
    function generateCatalog(selector) {

        // interop with multilangual 
        if ('' == 'true') {
            _containerSelector = 'div.post-container.active'
        } else {
            _containerSelector = 'div.post-container'
        }

        // init
        var P = $(_containerSelector), a, n, t, l, i, c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        // clean
        $(selector).html('')

        // appending
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#" + $(this).prop('id');
            t = $(this).text();
            c = $('<a href="' + i + '" rel="nofollow">' + t + '</a>');
            l = $('<li class="' + n + '_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    // toggle side catalog
    $(".catalog-toggle").click((function (e) {
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    /*
     * Doc: https://github.com/davist11/jQuery-One-Page-Nav
     * Fork by Hux to support padding
     */
    async("/js/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>



<!-- Multi-Lingual -->


<!-- Simple Jekyll Search -->
<script>
    // https://stackoverflow.com/questions/1912501/unescape-html-entities-in-javascript
    function htmlDecode(input) {
        var e = document.createElement('textarea');
        e.innerHTML = input;
        // handle case of empty input
        return e.childNodes.length === 0 ? "" : e.childNodes[0].nodeValue;
    }

    SimpleJekyllSearch({
        searchInput: document.getElementById('search-input'),
        resultsContainer: document.getElementById('search-results'),
        json: '/search.json',
        searchResultTemplate: '<div class="post-preview item"><a href="{url}"><h2 class="post-title">{title}</h2><h3 class="post-subtitle">{subtitle}</h3><hr></a></div>',
        noResultsText: 'No results',
        limit: 50,
        fuzzy: false,
        // a hack to get escaped subtitle unescaped. for some reason, 
        // post.subtitle w/o escape filter nuke entire search.
        templateMiddleware: function (prop, value, template) {
            if (prop === 'subtitle' || prop === 'title') {
                if (value.indexOf("code")) {
                    return htmlDecode(value);
                } else {
                    return value;
                }
            }
        }
    });

    $(document).ready(function () {
        var $searchPage = $('.search-page');
        var $searchOpen = $('.search-icon');
        var $searchClose = $('.search-icon-close');
        var $searchInput = $('#search-input');
        var $body = $('body');

        $searchOpen.on('click', function (e) {
            e.preventDefault();
            $searchPage.toggleClass('search-active');
            var prevClasses = $body.attr('class') || '';
            setTimeout(function () {
                $body.addClass('no-scroll');
            }, 400)

            if ($searchPage.hasClass('search-active')) {
                $searchClose.on('click', function (e) {
                    e.preventDefault();
                    $searchPage.removeClass('search-active');
                    $body.attr('class', prevClasses);  // from closure 
                });
                $searchInput.focus();
            }
        });
    });
</script>


<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
