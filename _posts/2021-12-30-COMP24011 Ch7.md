---
layout:     post
title:      人工智能导论 自然语言处理
subtitle:   NLP
date:       2021-12-30
author:     R1NG
header-img: img/post-bg-prolog.jpg
description: 本章将介绍自然语言处理的一般概念与信息检索和信息提取的一般方法.
catalog: true
tags:
    - 2021
    - 课程笔记
    - COMP24011
---

# 自然语言处理

本章将研究如何利用以自然语言描述的知识, 并介绍对自然语言进行信息检索和信息提取的一般方法. 

## 1. 语言模型

形如 `Java`, `Haskell` 的程序设计语言都有 **精确定义的语言模型**, 这种语言可以被定义为 **字符串的集合**, 并且可以 **通过一组称为 “语法” 的规则描述**. 形式语言也可通过规则来定义 **程序的意义或语义**. 

而自然语言, 如 `English` 或 `中文`, 则无法被描述为一个 **确定的语句集合**, 因此我们一般需要通过 **句子的概率分布** 而非 **集合** 定义自然语言模型. 

换言之, 给定某个字符串 “Axton is so strong!!!111”, 我们关注的是一个随机的句子恰为这个字符串的概率是多少, 而非考虑这个字符串是否属于定义语言的句子集合. 

同时, 自然语言具有歧义: 一个句子在不同的语境下可以解释为多个含义. 我们往往难以只用一个意义来解释某个句子, 而需要使用多个意义上的概率分布. 

最后, 需要说明的是, 自然语言的规模极其庞大, 并且时刻处于变化和演化之中. 因此, 我们的语言模型只能是 **对自然语言的模拟**. 

## 2. $n$ 元字符模型

本质上说, 自然语言的 **文本** 是由 **字符** 组成的, 如英语中的 **字母, 数字, 标点, 空格** 等. 因此, 最简单的语言模型就是 **字符序列的概率分布**. 

记 **从 $c_1$ 到 $c_N$ 共 $N$ 个字符构成一个文本序列** 的概率为

$$P(c_{1 : N}).$$

**定义 2.1** ($n$ 元组)
> 称 **长度为 $n$ 的书写符号序列** 为 **$n$ 元组** (`n-gram`) 的.
> 
> 注意, 我们分别用 `unigram`, `bigram` 和 `trigram` 表示 `一元组`, `二元组` 和 `三元组`.


**定义 2.2** ($n$ 元模型)
> 称 **$n$ 个字符序列上的概率分布** 为 $n$ 元模型.其中, 构成自负序列的元素不仅可以是字符, 也可以是 **单词**, **音节** 或其他单元.

**定义 2.3** (`Markov` 链)
> 我们可以简单地认为某一时刻状态的确定 **只依赖于在此之前的 $n$ 个状态** 的状态链为 $n$ 阶 `Markov` 链.

$n$ 元模型 (不只局限于 $n$ 元字符模型) 可以定义为 $n-1$ 阶 `Markov` 链. 显然, 在一个 **三元模型** (也就是 $2$ 阶 `Markov` 链) 中, 我们有:

$$P(c_i \vert c_{1 : i-1}) = P(c_i \vert c_{i-2: i-1}).$$

也就是说在三元模型中任何位置上的字符 $c_i$ 为某个字符的概率只由于它前面的两个位置上的字符决定.

在三元模型中, 我们结合 `链式规则` 和 `Markov 假设` 定义长为 $N$ 的字符序列的概率: 

$$P(c_{1:N}) = \prod_{i=1}^{N}P(c_i \vert c_{1:i-1}) = \prod_{i=1}^{N}P(c_i \vert c_{i-2:i-1}).$$

对于某个包含 $n$ 个字符的语言的三元字符模型, $P(c_i \vert c_{i-2:i-1})$ 有 $n^3$ 项参数. 我们可以通过对包含大量字符的文本集合进行精确的计数统计而得到, 我们称文本的集合为 **语料库** (`Corpus`). 

我们可以使用字符模型完成 **文本题材分类, 拼写纠错, 语言识别** 等任务. 

## 3. $n$ 元模型的平滑

$n$ 元模型的问题在于 **训练语料只提供了真实概率分布的估计值**, 而无法提供所有的边界情况. 因此, 我们需要采取措施使我们的语言模型能够有效地 **扩展到在语料库中从未见到过的文本上**. 

要实现这一点, 我们要通过改进语言模型使在训练文本库中出现概率为 $0$ 的字符序列仍能够被赋予一个 **很小的非零概率值**, 并将其他的数值同样小幅度下降使得概率的完备性仍然成立, 这一过程也被称为 **平滑**:

**定义 3.1** (平滑)
> 称通过赋予给定训练数据集中出现频率极低或未出现的数据一个较小的频率值, 从而使频率模型能够被扩展到一般情形的, 调整低频计数的过程为 **平滑**.

我们下面给出两张常用的简单平滑方法:

1. `Laplace` 平滑:
   
   `Laplace` 平滑指定, 若某个随机布尔变量 $X$ 在目前已有的 $n$ 个观察值中恒为 `false`, 则 
   
   $$P(X = \text{true}) = \frac{1}{n+2}.$$

   换言之, `Laplace` 平滑假定 **多进行两次试验**, 可能 **一个值为 `true`, 另一个为 `false`**.

2. 回退模型和线性插值平滑
   
   回退模型的基本思想是: 首先进行 $n$ 元计数统计, 若某些序列的统计值 **很低或者为零**, 则回退到 $n-1$ 元.

   线性插值平滑是一种通过线性插值将 **三元, 二元和一元模型** 组合起来的后退模型. 线性平滑插值定义概率估计值如下:

   $$\hat{P}(c_i ~\vert~ c_{i-2 : i-1}) = \lambda_{3} \cdot P(c_i ~\vert~ c_{i-2 : i-1}) + \lambda_{2} \cdot P(c_i ~\vert~ c_{i-1}) + \lambda_{1} \cdot P(c_i).$$

    此处系数满足

    $$\lambda_{3} + \lambda_{2} + \lambda_{1} = 1.$$

    参数值可以是固定的, 也可以通过某些算法进行计算. 关于参数值计算的讨论超出本文讨论的范围, 在此按下不表.

    在 $i=1$ 时, 表达式 
    
    $$P(c_i ~\vert~ c_{i-2 : i-1})$$

    就退化为 
    
    $$P(c_1 ~\vert~ c_{-1 : 0}),$$
    
    而在 $c_1$ 前并没有字符. 因此, 我们需要引入 **人工字符** 解决这一问题: 
    
    我们可以定义 $c_0$ 为空字符, 也可以回退到 **低价 `Markov` 模型**, 定义 $c_{-1 : 0}$ 为 **空序列**, 从而 
    
    $$P(c_1 ~\vert~ c_{-1:0}) = P(c_1).$$

$n$ 元字符模型可以被进一步扩展为 $n$ 元单词模型, 它检验的最小元素从 **字符** 变为了 **单词**. $n$ 元单词模型的平滑方法如下:

引入人工单词: 

人为定义训练数据集中 **每个首次出现的, 与之前遇到的所有单词都不同的单词** 为 **人工单词**, 指定在训练数据集中未出现的单词具有的概率为 **人工单词在训练数据集中出现的频率**. 

注意被定义为人工单词的单词不会发生改变, 如果 $a$ 在训练数据集中总共出现了 $x$ 次, 则它的第一个出现 $t$ 被指定为人工单词 $o$ **并不会使得它在训练数据集中出现的次数 $-1$**, 它的出现频率 $n$ 和不将其第一个出现指定为人工单词时的频率相同.

![20220104161240](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20220104161240.png)


## 4. 文本分类 (`Text Classification`)

我们下面讨论 **文本分类** (`Text Classification`) 问题, 其本质是 **给定一段文本, 判断它属于预定义的一系列类别中的哪一类**, 其实例包括 **情感分析** 和 **垃圾邮件检测**. 

归类的基本方法有两种: **语言模型方法** 和 **机器学习方法**. 

以垃圾邮件检测为例. 我们可以对垃圾邮件进行训练从而得到一个计算

$$P(\text{Message} ~\vert~ \text{spam})$$

的 $n$ 元语言模型, 并对正常邮件进行同样的训练, 得到计算

$$P(\text{Message} ~\vert~ \text{ham})$$

的模型. 随后, 我们可以应用 **贝叶斯规则** 对接收到的新消息进行分类:

$$\argmax_{c \in \{\text{spam , ham}\}} P(c ~\vert~ \text{Message}) = \argmax_{c \in \{\text{spam , ham}\}} P(\text{Message} ~\vert~ c) \cdot P(c).$$

注意此处 $P(c)$ 代表 **垃圾邮件或正常邮件在训练数据集中的频率**, 可以通过 **统计垃圾邮件和正常邮件的数目** 得到.

我们还可以使用机器学习方法: 将邮件信息视为一组 **特征-值** 对, 使用分类算法 $a$ 和特征向量 $X$ 进行判断. 

我们可以将 $t$ 元组作为特征: 比如 $t=1$ 时, 我们考虑一元模型, 此时 **在词汇表中的单词** 就是 **特征**, 而特征的 **值** 就是每个单词在这条邮件信息中出现的次数. 

我们称这样的 **一元表示形式** 为 **词袋模型** (`Bag of words`): 它就如同吧训练语料的单词放进一个袋子中, 每次从袋子里抽取一个单词从而构成邮件信息, 而需要注意的是, 该过程 **不保留词语之间的相互顺序**, 因此有 **信息丢失**.

在二元, 三元甚至更高元的模型中, 特征的数量成指数级别增长. 此外, 我们还可以加入邮件的一些 `metadata` 作为非 $n$ 元特征. 

在选定了特征集后, 我们就可以使用常见的监督学习技术训练文本分类模型.

## 5. 信息检索 (`Information Retrieval`)


