---
layout:     post
title:      人工智能导论 不确定性的量化和概率推理
subtitle:   Uncertainty & Bayes Network
date:       2021-12-20
author:     R1NG
header-img: img/post-bg-prolog.jpg
description: 本章将介绍Agent如何对环境中的不确定性进行量化并使用概率推理感知非确定性的环境.
catalog: true
tags:
    - 2021
    - 课程笔记
    - COMP24011
---

# 不确定性的量化和概率推理

在本章中, 我们将借助概率论考虑 `Agent` 在不确定环境下的行动问题, 并引入贝叶斯网络构造 `Agent` 在不确定性下进行推理的网络模型. 

## 4.1 不确定性

我们首先对 **不确定性问题** 给出定义. 

**定义 4.1.1** (不确定性)
> 我们称满足下列特性的问题为 **不确定性问题**:
> 1. 要得到该问题的确定没有任何意外情况的规则需要列出前提和结论的完整集合, 而为了列出这个集合需要极为庞大的工作量, 给出的规则也难以使用.
> 2. 对于问题涉及的领域, 并不存在完整的结论能够支撑我们给出完备的规则.
> 3. 即便在给出了完备规则的前提下在一些情形下我们也无法进行完整的测试, 因此不能通过逻辑联系得出结论.

在具有确定性的问题中, 世界是由一系列 **在某种特定情形下, 或成立或不成立的事实** 构成的, 而在这样的不确定性问题下, `Agent` 的知识只能提供对相关语句的 **信念度 (`Degree of belief`)**: 也就是一个介于 $0-1$ 之间的数值, 作为对这一语句实际发生的可能性的信念, 本质上是概率值, 因而我们需要使用概率论处理信念度. 

在真实世界中, 实际上并不具备 “不确定性”. 对 `Agent` 而言, 概率声明之所以具备不确定性是因为它对现实世界的了解是有限的, 只能够通过已知的一系列有限的知识状态作出推断. 

## 4.2 概率语法规则

为了让 `Agent` 表示并使用概率信息, 我们下面给出一种 **形式语言** 的定义. 

逻辑断言所考虑的是要严格地排除那些断言不成立的情形, 而概率断言考虑的是各种情形发生的可能性. 

所有可能情形所组成的集合称为 **样本空间**, 样本空间中的不同情形是 **互斥的** 和 **完备的**, 每个情形 $\omega$ 都被赋予一个对应的数值概率 $P(\omega)$. 并且满足:

$$\text{for each }\omega, ~~0 \leqslant P(\omega) \leqslant 1, ~~ \sum_{\omega \in \Omega} P(\omega) = 1.$$

我们称 **先验概率 (`Prior probabilities`)** (或无条件概率) 为 **在不知道其他信息的情况下对命题的信念度**, 而认定 **后验概率 (`Posterior probabilities`)** (或条件概率) 为 **给定一些已知的信息 (证据) 的情况下对命题的信念度**. 

我们将结合 **命题逻辑中的元素** 和 **约束满足问题语法中的记号** 描述命题. 在概率论中, 变量被称为 **随机变量 (`Random Variables`)**, 而每个随机变量都具有一个 **定义域** (`Domain`), 由该随机变量能取的所有值组成. 随机变量本身可以表示基本命题, 而将基本命题用命题逻辑中的逻辑链接符相连就组成了更为复杂的命题. 如, 我们可以将 “如果一个人叫 Axton, 他又很强, 那么他是姚老师的概率是 $0.9$” 表示为:

$$P(\text{isAxtonYao} ~ \vert ~\neg \text{isWeak} ~\wedge~ \text{isAxton}) = 0.9.$$

我们还可以使用逗号分隔多个变量, 表示多个变量的分布. 变量 `Weather` 和 `Cavity` 的所有可能取值的乘法规则可以被精炼为下述的单一等式

$$P( \text{Weather}, \text{Cavity}) = P( \text{Weather} ~\vert~ \text{Cavity}) \cdot P(\text{Cavity}).$$

## 4.3 使用完全联合分布进行推理

我们可以通过 **已观察到的证据计算命题的后验概率**: 使用完全联合概率分布作为 “知识库”, 就可以从中导出所有相关问题的答案:

考虑一个由三个布尔变量 $\text{Toothache}, \text{Cavity}$ 和 $\text{Catch}$ 组成得分问题域, 其完全联合分布为一个 $2*2*2$ 的表格:

![20211231222326](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20211231222326.png)

1. 首先, 最常见的任务是 提取关于随机变量的 **某个子集** 或 **某个变量** 的概率分布, 这样的过程一般称为 **边缘化** (`Marginalization`) 或 **求和消元** (`Summing Out`). 
    
    对任何两个变量集合 $Y, Z$, 有下列的 **通用边缘化规则**: 
    
    $$P(Y) = \sum_{z \in Z} P(Y, z)$$

    其中 $\sum_{z \in Z}$ 指针对变量集合 $Z$ 的所有可能取值组合进行求和. 我们还可以使用条件概率将上述规则变形如下:

    $$P(Y) = \sum_{z \in Z} P(Y \vert z) P(z).$$

2. 在一些情况下, 我们还需要基于一些已知的变量证据而计算另一些变量的条件概率. 设 $E$ 为证据变量集合, $e$ 为其观察值, 并设 $Y$ 为其余的未观测变量, 查询为 $P(X \vert e)$, 则其值为:

    $$P(X \vert e) = \alpha P(X, e) = \alpha \cdot \sum_{y \in Y} P(X, e, y).$$

    其中 $\alpha = P(e)$, 对于 $\forall y \in Y$ 是不变的, 因此可以视为一个归一化常数. 

## 4.4 应用贝叶斯规则

从概率的乘法规则和条件概率公式可得:

$$P(b \vert a) = \frac{P(a \vert b) P(b)}{P(a)}.$$

对于多值变量的更一般情况, 可以表为

$$P(Y \vert X) = \frac{P(X \vert Y) P(Y)}{P(X)}.$$

## 4.5

