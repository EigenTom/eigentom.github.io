---
layout:     post
title:      自然语言处理:词义消歧
subtitle:   Word Sense Disambiguation
date:       2022-10-22
author:     R1NG
header-img: img/post-bg-comp34711.jpg
description: 本章讨论词义消歧问题.
catalog: true
tags:
    - 课程笔记
    - 自然语言处理
---

# 词义消歧

本章我们讨论自然语言处理中的 **词义消岐** (`Word Sense Disambiguation`) 问题. 在给出该问题的基本定义后, 我们将依序介绍多种处理方法.

## 基本定义

首先定义 **词义**. 由于单词可能有 **多种不同的意思**, 因此我们认为单词的 **词义** 是它在 **特定语境环境下** 的 **特定含义**. 

而 **词义消岐** 任务的实质就是: 对于 **给定单词** 而言, 从该单词 **已知的一系列可能的含义** 里, 在一段 **给定的文本** 中选出 **契合上下文** 的那个单词含义. 

![20221108155122](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108155122.png)

词义消岐在 **机器翻译**, **信息提取** 等领域中都有广泛应用. 解决词义消岐问题的基本方法可以被大致分为两类:

1. 基于知识的解决方案: 使用如语料库, 字典等 **外部词汇资源** (`external lexical resources`) 进行词义的选择.

2. 基于监督学习的解决方案: 使用预先标注好的训练语料 (`Training Samples`) 训练用于选择词义的 **机器学习模型**.

下面我们介绍一系列不同的词义消岐算法.

## `Lesk` 算法及其变体

`Lesk` 算法是一种 **经典的, 基于知识 (`Knowledge-Based`) 的词义消岐算法**. 它有两种变体: `Lesk Algorithm` 和效率稍高的 `Simplified Lesk Algorithm`.

### `Lesk Algorithm`

`Lesk Algorithm` 的核心思想是: 对给定文本中的 **每一个字词**, 检查 **所有字词的不同含义之间的所有组合**, 取 **字词含义组合 (也是句子) 中找到重复字词最多的**, 把这个含义组合作为算法的结果, **同时确定文本中每一个字词的含义**.

考虑下面的例子:

![20221108160503](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108160503.png)

显然, 给定文本由两个字词: `pine` 和 `cone` 组成, 它们分别有 $2$ 种和 $3$ 种含义. 

基于 `Lesk Algorithm` 的假设, 我们需要考虑 $2 \cdot 3 = 6$ 种含义组合, 对每个含义组合, 我们都需取出对应含义的描述, 也就是一个个 **句子**, 然后 **统计这些句子之间重复出现的单词数量有多少**.

因此可以得到下面的表和结果:

![20221108160720](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108160720.png)

我们可以得到下面的总结:

![20221108160837](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108160837.png)

可见 `Lesk` 算法的主要问题是计算复杂度几乎随句子长度呈指数级增加, 这主要是因为它在计算过程中同时计算了一些其实无需计算的, 其他所有字词的可能含义. 为了优化这一问题, 后来有给出了 **简化的 `Lesk` 算法**:

### `Simplified Lesk Algorithm`

`Simplified Lesk` 算法的基本思想不再是考虑语料中每个单词的每种可能含义的交集, 而只考虑 **给定单词的每种可能含义** 和 **语料本身** 的 **交集**:

![20221108162535](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108162535.png)

这样, 即使需要计算 **给定语料中每个单词的含义**, 计算复杂度也就从常规 `Lesk` 算法的指数级降低到了常数级.

![20221108162622](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108162622.png)

### `Corpus Lesk`

基于语料库的 `Lesk` 算法 (`Corpus Lesk`) 在简化的 `Lesk` 算法基础上引入了 **经过标记的语料库** 以提升词义鉴别的准确性. 

语料库的引入带来了两点好处: 

1. 我们可以引入 **给定单词, 特定意义的更多使用例**, 从而将这个含义的更多特殊使用场景都考虑在内, 提升词义鉴别的准确性.
2. 同时可以对 (语料库和给定文本中) 可能发生重叠的单词赋予 **不同的权重**, 以此关注 **那些更能用于确定给定单词的含义的词**, 而非 **一些意义相对不大的词, 如停止词**, 进一步提升词义鉴别的准确性.

![20221108163117](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108163117.png)

在实践中, 我们可以使用语料库中单词的 `idf` 作为重叠单词的权重.

![20221108163258](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108163258.png)

## 基于分类的词义消歧

我们下面考虑 **基于机器学习方法** 的词义消岐算法.

基于分类的词义消岐将词义消岐问题建模为机器学习领域中常见的 **分类** 问题. 它以 **需要被鉴别含义的字词** 作为模型的输入, 以 **这个字词可能被取到的所有含义** 作为分类问题中一系列预先给定的不同分类, 以 **一系列在上下文中被标记了对应含义** 的字词作为模型的训练语料库.

可以考虑下面的例子:

![20221108191849](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108191849.png)

![20221108191928](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108191928.png)

然后就可以使用一系列用于分类模型的方法与技术进行分类算法的选择, 训练优化和评估. 

![20221108192046](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108192046.png)

## 基于特征提取的词义消歧

此外, 我们还可以利用上一章中所讨论的 **特征提取** 相关的技术和知识进行词义消岐.

换言之, 我们可以通过 **借用词袋模型和窗宽的概念**, 考虑给定文本和语料库中, **以目标词为中心**, 给定窗宽范围内出现的 **所有词**, 并对这些词的 **词频** 进行统计, 从而判断给定文本中的目标词含义应该是哪一个.

考虑下面的例子:

![20221108214009](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108214009.png)

在固定窗宽为 $2$ 的情况下, 可以从给定语料中提取出黄色高亮标记的 **词袋特征**, 然后就可以将这个特征转换为 **这个文本对应的特征向量** (`Feature Vector`).

进一步地, 我们可以引入一个由有限个单词组成的 **词汇表** (`Vocabulary List`), 将词汇表中的每个单词确定为特征向量的每一个维度, 然后就可以将文本中不同单词的 **出现次数** 作为对应维度的值, 如下所示:

![20221108214321](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108214321.png)

然后通过比对特征向量的 **相似度**, 就可以做出对应的判断.

## 基于朴素贝叶斯的词义消岐

我们还可以利用 **概率** 思想, 构造基于 **朴素贝叶斯定理** 的词义消岐算法.

一般地, 若计算出特征向量 $x$ 关于某个分类 $y$ 所对应的 **判别函数值**, **先验概率**, **条件概率** 和 **联合概率**, 就可以使用朴素贝叶斯定理基于概率值的不同构造分类器.

![20221108215059](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108215059.png)

考虑上面的例子. 对每个不同的可能分类 $y_i$, 关于作为输入的特征向量 $x$, 目标是计算出所有的 **联合概率 $p(x, y_i)$**, 然后取对应联合概率值最高的那个分类, 作为应得的分类.

而基于朴素贝叶斯定理可知, 对联合概率的计算就是对条件概率的计算. 因此有: 

![20221108215112](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108215112.png)

![20221108215144](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108215144.png)

进一步地可以使用词频统计对条件概率进行 **估计**:

![20221108215235](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108215235.png)

最后在上面的例子中即可通过计算得出哪个类别对应的联合概率数值最大, 它也就是分类器应该返回的结果:

![20221108215330](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108215330.png)

## 将词义消岐任务转换为序列标注任务

**序列标注** (`Sequence Labeling`) 任务就是将一系列标签分别赋予一系列数据的任务. 我们可以通过训练合适的机器学习模型完成序列标注任务: 将作为输入的序列 **映射到** 某个作为输出的新序列上. 

同时可见, 由于在词义消岐任务中存在 “对目标词赋予表示它在文中含义的标签” 这个步骤, 我们可以很自然地将词义消岐任务转换为序列标注任务.

![20221108221528](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108221528.png)

序列标注任务可以使用 **结构化支持向量机**, **条件随机场**, **隐马尔可夫链** 和 **循环神经网络** 等算法解决. 下面简要叙述几种模型. 

### 隐马尔可夫链

### 循环神经网络

(此处日后有时间补)

## 基于自展取样的数据增强

在训练数据不足时, 可以使用 **自展取样** (`BootStrapping`), 基于现存的一小部分数据生成一系列新的数据.

在词义消岐任务的数据增强中, 生成新数据的主要方法是 **基于一系列规则** 生成新数据. 

常见的两个规则如下:

1. "One sense per collocation": 与同一个词搭配出现的词几乎肯定具有相同的含义.
2. “One sense per disclose”: 在同一个具有相同 **主题** 的文档中的词义高度一致. 

通过使用 **数据生成规则**, 结合人为标记的现存数据, 就可以生成一系列新的训练数据. 这种自展取样方法可被归类为 **半监督学习** (`Semi-supervised learning`).

## 对词义消岐任务的表现评估

![20221108221739](https://cdn.jsdelivr.net/gh/KirisameR/KirisameR.github.io/img/blogpost_images/20221108221739.png)