---
layout:     post
title:      数据分析方向抱佛脚八股
subtitle:   就你0准
date:       2023-07-14
author:     R1NG
header-img: img/post-bg-algorithm.jpg
description: 数据分析方向抱佛脚八股，包含SQL， Python，统计学和产品测试相关知识点。
catalog: true
tags:
    - 奇技淫巧
    - 人工智障
---

# 数据分析方向抱佛脚八股

## 1. SQL

基本语法：
增 - INSERT INTO
删 - DELETE FROM 表 WHERE 条件
改 - UPDATE 表 SET 列名 = 新值
查 - SELECT 列名 FROM 表 WHERE 条件

Where 子句的条件：
1. 比较运算符：=, <>, !=, >, <, >=, <=
2. 逻辑运算符：AND, OR, NOT
3. 范围运算符：BETWEEN, IN, LIKE
4. 列表运算符：ANY, ALL, EXISTS, IN，NOT IN, SOME
5. 字符运算符：LIKE, NOT LIKE
6. 判空运算符：IS NULL, IS NOT NULL
7. 时间运算符：BETWEEN, DATE_ADD, DATE_SUB, DATE_DIFF, DATE_FORMAT, DATE, DAY, DAYNAME, DAYOFMONTH, DAYOFWEEK, DAYOFYEAR, EXTRACT, HOUR, MINUTE, MONTH, MONTHNAME, NOW, QUARTER, SECOND, TIME, TIMESTAMP, WEEK, YEAR
8. 判重复运算符：DISTINCT, DISTINCTROW

注意： AND 和 OR 的优先级不同，AND 优先级高于 OR，可以使用括号来改变优先级。

模糊查询：
1. %：表示任意个字符
2. ^ 或 !：表示否定
3. []：表示范围
4. _：表示单个字符

排序：

ORDER BY：默认升序（ASC），DESC 降序，可连续基于多个key排序。

聚合函数：
1. COUNT：计数 （不为NULL）
2. MAX：最大值
3. MIN：最小值
4. SUM：求和
5. AVG：平均值

分组 GROUP BY：
1. GROUP BY 语句用于结合聚合函数，根据一个或多个列对结果集进行分组。
2. SQL在实际执行中会先进行筛选（WHERE）再进行分组（GROUP BY），最后进行聚合。

连表查询：（主要考虑INNER JOIN）
1. INNER JOIN：内连接，两表中都有的数据才会被保留。
2. LEFT JOIN：左连接，左表中的数据都会被保留，右表中没有的数据用NULL填充。
3. RIGHT JOIN：右连接，右表中的数据都会被保留，左表中没有的数据用NULL填充。
4. OUTER JOIN（FULL JOIN）：外连接，左右表中的数据都会被保留，没有的数据用NULL填充。
5. 自链接：自身连接，将表自身连接，一般用于树形结构的查询。

注： MySQL没有FULL JOIN，但是可以通过UNION实现。

集合运算 UNION：
1. UNION：取并集，去重。
2. UNION ALL：取并集，不去重。

条件 HAVING：
1. HAVING 子句可以让我们筛选分组后的各组数据。
2. Where和Having的区别：
    - Where在分组前进行筛选，Having在分组后进行筛选。
    - Where后不能使用聚合函数，Having后可以使用聚合函数。
    - Where后不能使用分组后的列，Having后可以使用分组后的列。

条件分支 CASE：
1. 基本结构：
    ~~~sql
    CASE 
        WHEN 条件1 THEN 结果1 
        WHEN 条件2 THEN 结果2 
        ELSE 结果3 
    END
    ~~~
2. 可以嵌套使用。
3. 可用于基于条件的搜索。

范围取值 IN：
1. IN：取多个值。
2. NOT IN：取不在范围内的值。

窗口函数：
1. 窗口函数是一类特殊的函数，它可以在不改变查询结果的情况下，对查询结果进行排序、分组、求TOPN等操作。
2. 窗口函数的语法：
    ~~~sql
    <窗口函数> OVER ([PARTITION BY <列名>]
    ORDER BY <列名> [ASC|DESC])
    ~~~
3. 常见的窗口函数及其用途：
    - ROW_NUMBER()：为每一行添加一个序号，常用于分页。
    - RANK()：为每一行添加一个序号，相同的值会得到相同的序号，但是不会跳过序号。
    - DENSE_RANK()：为每一行添加一个序号，相同的值会得到相同的序号，但是会跳过序号。
    
    注意： 在上述的三个函数中，如果不指定ORDER BY，则会按照默认顺序进行排序，括号中不需要任何参数。
    
    - NTILE()：将数据分成n份，返回每一行所在的份数。
    - FIRST_VALUE()：返回分组中第一行的值。
    - LAST_VALUE()：返回分组中最后一行的值。
    - LAG()：返回当前行前n行的值。
    - LEAD()：返回当前行后n行的值。

数据库范式 1NF 2NF 3NF：
1. 1NF：每一列都是不可分割的原子数据项。
2. 2NF：在1NF的基础上，非主属性完全依赖于主属性，不存在某个非主属性只依赖于主属性的一部分的情况。
3. 3NF：不存在所谓的 “传递依赖”：A->B, B->C, 则A->C。

Count()和Count(1)与Count(*)的区别：
   1. Count(字段)：统计列中出现该字段的非空的行数，不统计字段为NULL的情况。
   2. Count(1)：统计列中第一个子字段的行数，统计NULL。 里面的1表示代码行，可以统计表中所有数据。
   3. Count(*)：统计所有的列， 也就是统计行数，包括NULL。

执行效率：
   1. 如果列为主键，count（列名）效率优于count
   2. 如果列不为主键，count（1）效率优于count（列名）
   3. 如果表中存在主键，count（主键列名）效率最优，
   4. 如果表中只有一列，则count（星号）效率最优，
   5. 如果表中有多列，且不存在主键，则count（1）效率更优


下面介绍数个高频SQL题目：

1. 使用窗口函数筛选出连续三天登录的用户
    ~~~sql
    SELECT DISTINCT user_id FROM
    (
        SELECT 
            user_id, 
            login_date, 
            1 AS rn 
        FROM login_log
    ) 
    
    AS t

    GROUP BY 
        user_id, 
        date_sub(login_date, INTERVAL rn DAY)

    HAVING COUNT(rn) >= 3 
    ~~~

2. SQL行转列：
    ~~~sql
    SELECT * FROM student PIVOT (MAX(score) FOR subject IN ('语文', '数学', '英语'))
    ~~~

3. SQL列转行：
    ~~~sql
    SELECT * FROM student UNPIVOT (score FOR subject IN ('语文', '数学', '英语'))
    ~~~

4. SQL分组求TopN：
    ~~~sql
    SELECT * FROM (
        SELECT 
            user_id, 
            login_date, 
            ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY login_date DESC) AS rn
        FROM login_log
    ) AS t
    WHERE rn <= 3
    ~~~

    注： 使用 UNION 也可以得到相同的效果。

参考： 

1. https://zhuanlan.zhihu.com/p/581526034
2. https://www.cnblogs.com/BRSblackshoot/p/15562434.html


## 2. Python

Python 基本数据类型（list, dict, tuple, set, deque）及其差别：
   1. list：列表，有序，可重复，可变。
   2. dict：字典，无序，可重复，可变，键值对。
   3. tuple：元组，有序，可重复，不可变。
   4. set：集合，无序，不可重复，可变。
   5. deque：双端队列，有序，可重复，可变。

如何使用 sklearn 进行数据正则化？
    1. 使用 sklearn.preprocessing.StandardScaler 进行标准化。
    2. 使用 sklearn.preprocessing.MinMaxScaler 进行归一化。
    3. 使用 sklearn.preprocessing.RobustScaler 进行缩放。
   
如何使用 sklearn 进行数据降维？
    使用 sklearn.decomposition.PCA 进行主成分分析。



## 3. 统计学

1. 什么是混杂变量/混杂因素？（Confounding Variable）混杂变量是指在研究中，与自变量和因变量都有关系的变量。

2. 什么是协变量？（Covariate）协变量是指在研究中，与自变量有关系的变量。

3. 什么是辛普森悖论？（Simpson's Paradox）辛普森悖论是指在研究中，当我们忽略了混杂变量的影响，就会得到错误的结论。

4. 什么是费米问题？（Fermi Problem）费米问题是指在研究中，当我们缺乏数据时，可以通过一些简单的假设，来估算出一个大致的结果。

5. 什么是假设检验？（Hypothesis Testing）假设检验是指在研究中，我们需要对研究结果进行统计检验，以判断研究结果是否具有统计学意义。

6. 常见的假设检验法有哪些？（T检验、方差分析、卡方检验、KS检验、Wilcoxon秩和检验、Mann-Whitney U检验、Kruskal-Wallis H检验、Friedman检验）

7. 什么是置信区间？（Confidence Interval）置信区间是指在研究中，我们可以通过置信区间来估算出一个范围，这个范围内包含了真实值的概率。

8. p值是什么？（P-value）p值是指在研究中，我们可以通过p值来判断研究结果是否具有统计学意义。给定一个原假设，p值是指在原假设 （当前数据符合给定的分布）成立的情况下，得到当前样本或更极端样本的概率。

9. 什么是置信水平？（Confidence Level）置信水平是指在研究中，我们可以通过置信水平来判断研究结果是否具有统计学意义。置信水平是指在原假设成立的情况下，得到当前样本或更极端样本的概率。

10. 什么是大数定律？（Law of Large Numbers）大数定律是指在研究中，当样本数量足够大时，样本均值会无限接近于总体均值。

11. 什么是中心极限定理？（Central Limit Theorem）中心极限定理是指在研究中，当样本数量足够大时，样本均值的分布会无限接近于正态分布。

12. 什么是方差？（Variance）方差是指在研究中，用来衡量数据的离散程度。

13. 什么是平均数？（Mean）平均数是指在研究中，用来衡量数据的集中程度。

14. 什么是众数？（Mode）众数是指在研究中，出现次数最多的数值。

15. 什么是中位数？（Median）中位数是指在研究中，将数据从小到大排序后，位于中间位置的数值。

16. 如何计算T检验的p值？（T-test）T检验是指在研究中，用来检验两组数据是否具有统计学意义。

17. 其中，T检验的p值可以通过T分布表来计算，也可以通过Python的scipy库来计算。

    计算公式如下：
    $$
    t = \frac{x_1 - x_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}
    $$

    其中，x1和x2分别是两组数据的均值，s1和s2分别是两组数据的标准差，n1和n2分别是两组数据的样本数量。

    通过T分布表可以得到T值对应的p值，也可以通过scipy库中的ttest_ind函数来计算。

    ~~~python
    from scipy import stats

    t, p = stats.ttest_ind(x1, x2)
    ~~~

18. 什么是协方差？（Covariance）协方差是指在研究中，用来衡量两组数据的相关性。

    其计算公式为？(sample covariance)

    $$
    cov(x, y) = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{n-1}
    $$

    或 (population covariance)

    $$
    cov(x, y) = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{n}
    $$

    其中，x和y分别是两组数据，n是样本数量，$\bar{x}$和$\bar{y}$分别是两组数据的均值。

19. sample covariance 和 population covariance的区别是什么？（sample covariance是样本协方差，population covariance是总体协方差）

20. 什么是相关系数？（Correlation Coefficient）相关系数是指在研究中，用来衡量两组数据的相关性。

    其计算公式为：

    $$
    r = \frac{cov(x, y)}{s_x s_y}
    $$

    其中，x和y分别是两组数据，$s_x$和$s_y$分别是两组数据的标准差。

21. 什么是Pearson相关系数？（Pearson Correlation Coefficient）Pearson相关系数是指在研究中，用来衡量两组数据的相关性。

    其计算公式为：
    $$
    r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2 \sum_{i=1}^{n}(y_i - \bar{y})^2}}
    $$

    其中，x和y分别是两组数据，n是样本数量，$\bar{x}$和$\bar{y}$分别是两组数据的均值。

22. 


## 4. 产品测试

1. 什么是EDA？（Exploratory Data Analysis）探索性数据分析。

2. 什么是A/B测试？（A/B Testing）又称为对照实验，是一种统计学上的实验设计，用于检验两种不同的处理方式是否有显著差异。

3. 如何选择A/B测试的指标？（主要考虑指标的可测量性、可操作性、可信度）

4. 什么是漏斗模型？（Funnel Model）漏斗模型是一种分析用户行为的模型，它将用户行为分为不同的阶段，每个阶段都有一个转化率，通过分析每个阶段的转化率，可以找到用户流失的原因。

5. 什么是灰度发布？（Gray Release）灰度发布是指在发布新版本时，只将新版本的功能部署给部分用户，如果没有问题，再将新版本的功能部署给所有用户。

6. 灰度发布和A/B测试的区别？（灰度发布是一种上线方式，A/B测试是一种验证方式）

7. 什么是冷启动？（Cold Start）冷启动是指在产品刚上线时，由于没有用户数据，无法进行个性化推荐的问题。

8. 如何解决冷启动问题？（主要有三种方法：基于内容的推荐、基于热门的推荐、基于用户的推荐）

9. 什么是用户画像？（User Profile）用户画像是指通过分析用户的属性、行为、兴趣等信息，对用户进行描述的过程。

10. 用户画像的应用场景？（主要有三个方面：个性化推荐、精准营销、用户分群）

11. 什么是精准营销？（Precision Marketing）精准营销是指通过分析用户的属性、行为、兴趣等信息，对用户进行描述，然后根据用户的特征，向用户推送相关的广告。

12. 什么是用户分群？（User Segmentation）用户分群是指将用户划分为不同的群体，然后针对不同的群体，采取不同的营销策略。

13. 如何构建用户画像的标签体系？（主要有三个步骤：确定标签的维度、确定标签的类型、确定标签的取值）

    具体步骤如下：

    1. 确定标签的维度：用户画像的标签维度主要包括用户的属性、行为、兴趣等信息。

    2. 确定标签的类型：用户画像的标签类型主要包括用户的基本属性、用户的行为属性、用户的兴趣属性等。

    3. 确定标签的取值：用户画像的标签取值主要包括用户的基本属性取值、用户的行为属性取值、用户的兴趣属性取值等。

14. 什么是ROI？（Return on Investment）投资回报率。计算方法：ROI = (收益 - 成本) / 成本。

15. 什么是DAU？（Daily Active User）日活跃用户。计算方法：DAU = 日活跃用户数 / 日总用户数。

16. 什么是GMV？（Gross Merchandise Volume）成交总额。计算方法：GMV = 成交总额。

17. 什么是RFM模型？（Recency、Frequency、Monetary）RFM模型是一种用户价值分析模型，它将用户的价值分为三个维度：最近一次消费时间、消费频率、消费金额。

18. 对数据分析的认识？（数据分析是指通过分析数据，找到数据背后的规律，从而为产品决策提供支持。）

19. 业务分析方法包括但不限于：
    1. 周期分析；
    2. 趋势分析；
    3. 结构分析（维度拆解，公式拆解，MECE原则）；
    4. 分层分析；
    5. 象限分析；
    6. 同期群分析；
    7. RFM模型；
    8. AB测试；
    9. 归因分析与技术；
    10. UE模型；
    11. 画像分析；
    12. 帕累托分析；
    13. 用户研究；
    14. 行业研究；
    15. 竞品分析等。

# 机器学习

1. 简述PCA的定义及其原理。
   
   PCA （主成分分析）是一种常用的数据分析方法，它通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量称为主成分。

   它利用了协方差矩阵的特征值和特征向量来实现降维，具体步骤如下：
   
   1. 计算协方差矩阵；
   2. 计算协方差矩阵的特征值和特征向量；
   3. 从大到小对特征值排序，选择前k个特征值对应的特征向量，特征值越大，表示该特征向量包含的信息越多；（和其他维度数据的相关性越小）
   4. 使用所选择的特征向量构造变换矩阵；

   优点：精度较高，不受噪声干扰，适用于无监督式学习，缺点：计算量大，公式不直观，难理解，且特征值必须是数值型。

    
2. 简述数据归一化 （Normalization）的三种主要类别及其区别：
    1. 标准差标准化 （StandardScaler）：
        
        将数据转换为均值为0，标准差为1的正态分布；
    
        转化函数：

        $$x' = \frac{x - \mu}{\sigma}$$

        适用性：适用于本身服从正态分布的数据，或者对数据的分布没有要求的情况。
    
    2. 最大最小值标准化 （MinMaxScaler）：
        
        将数据转换为[0,1]区间上的均匀分布；
        
        转化函数：

        $$x' = \frac{x - min}{max - min}$$

        适用性：适用于数据分布较稳定的情况。
    
    3. 稳健标准化 （RobustScaler）：
        
        将数据转换为中位数为0，四分位数为1的分布；
        
        转化函数：

        $$x' = \frac{x - Q_2}{Q_3 - Q_1}$$

        其中，$Q_1$为下四分位数，$Q_2$为中位数，$Q_3$为上四分位数。

        适用性：适用于包含许多异常值的数据。
    
    总结：
    1. 在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，StandardScaler表现更好（避免不同量纲对方差、协方差计算的影响）；
    
    2. 在不涉及距离度量、协方差、数据不符合正态分布、异常值较少的时候，可使用MinMaxScaler。（eg：图像处理中，将RGB图像转换为灰度图像后将其值限定在 [0, 255] 的范围）；

    1. 在带有的离群值较多的数据时，推荐使用Ro bustScaler。

    参考：https://smilecoc.vip/2020/09/03/sklearn三种数据标准化方法的对比：StandardScaler、MinMaxScaler、RobustScaler/

3. 简述决策树和XGBoost的原理：
   决策树是一种基于树结构的机器学习算法，它通过对数据进行多次二分，构建一棵树，从而实现对数据的分类。

   决策树适用于表格型数据，它的优点是易于理解，缺点是容易过拟合。为了避免过拟合，可以通过剪枝、随机森林等方法来提高模型的泛化能力。

   随机森林是一种基于决策树的集成学习算法，它通过对多棵决策树的结果进行投票，从而实现对数据的分类。随机森林中每棵树的构建过程中，都会随机选择一部分特征，这样可以避免某些特征对模型的影响过大，从而提高模型的泛化能力。

    XGBoost是一种可扩展的，基于决策树的集成学习算法，它通过对多棵决策树的结果进行加权求和，从而实现对数据的分类。XGBoost中每棵树的构建过程中，都会考虑上一棵树的结果，也就是将前一棵决策树的偏差考虑在内，这样可以提高模型的泛化能力。

    生成每棵决策树使用的数据集都是整个数据集。

    一棵完整的树的生成步骤：
    1、基于贪心算法进行划分，通过计算目标函数增益，选择该结点使用哪个特征

    2、为了提高算法效率，使用“加权分位法”，计算分裂点。只考虑计算分裂点的目标函数值，而不是考虑所有特征值

    3、可以选择“全局策略”还是“局部策略”计算分裂点

    参考： 
    1. https://zhuanlan.zhihu.com/p/562983875
    2. https://zhuanlan.zhihu.com/p/290964953

4. 对机器学习模型评价的基本指标有哪些？ （混淆矩阵，Precision，Accuracy，Recall，F-1，ROC，AUC）
    1. 混淆矩阵（Confusion Matrix）：
        
        混淆矩阵是一种可视化的评价模型的方法，它可以直观地展示模型的分类效果。

        混淆矩阵的行表示真实类别，列表示预测类别，对角线上的元素表示预测正确的样本数，非对角线上的元素表示预测错误的样本数。

        混淆矩阵的元素可以用来计算其他评价指标，如Precision，Recall，F-1等。

    2. Precision：
        
        Precision表示预测为正的样本中，真正为正的样本的比例。

        $$Precision = \frac{TP}{TP + FP}$$

        其中，TP表示真正为正的样本数，FP表示预测为正的样本中，实际为负的样本数。

        Precision越高，表示模型预测为正的样本中，真正为正的样本越多，模型的分类效果越好。

    3. Accuracy：
        
        Accuracy表示预测正确的样本数占总样本数的比例。

        $$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$

        其中，TP表示真正为正的样本数，TN表示真正为负的样本数，FP表示预测为正的样本中，实际为负的样本数，FN表示预测为负的样本中，实际为正的样本数。

        Accuracy越高，表示模型预测正确的样本数越多，模型的分类效果越好。

    4. Recall：
        
        Recall表示真正为正的样本中，预测为正的样本的比例。

        $$Recall = \frac{TP}{TP + FN}$$

        其中，TP表示被识别为正的正样本数，FN表示被识别为负的正样本数。
    
    5. F-1：
        本质是Precision和Recall的调和平均数，它可以同时考虑Precision和Recall，从而综合评价模型的分类效果。

        $$F1 = \frac{2 * Precision * Recall}{Precision + Recall}$$
    
    6. ROC：
        ROC曲线是一种可视化的评价模型的方法，它可以直观地展示模型的分类效果。

        一般地，ROC曲线的横轴表示False Positive Rate（FPR），纵轴表示True Positive Rate（TPR）。曲线越接近右上角（包围的面积趋向于正方形），表示模型的分类效果越好。
    
    L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择.
    L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合.


    添加了BN层能加快模型收敛，一定程度上还有的dropout的作用。

    BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。

    IN和BN最大的区别是，IN作用于单张图片，BN作用于一个batch。IN多适用于生成模型中，例如风格迁移。像风格迁移这类任务，每个像素点的信息都非常重要，BN就不适合这类任务。BN归一化考虑了一个batch中所有图片，这样会令每张图片中特有的细节丢失。IN对HW做归一化，同时保证了每个图像实例之间的独立。

    mae rmse：measure error magnitude，latter offer a higher punishment on large errors，useful for the circumstance where large error is undesirable。

    交叉熵损失函数：本质上也是一种对数似然函数，可用于二分类和多分类任务中

    当使用sigmoid作为激活函数的时候，常用交叉熵损失函数而不用均方误差损失函数，因为它可以完美解决平方损失函数权重更新过慢的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。

    sigmoid：输出梯度分布中心不为0，容易导致输出的梯度值均偏向positive或negative，在神经网络中传导，造成学习速率下降，学习缓慢；在梯度为正负无穷方向梯度更新近乎为0，容易导致梯度饱和，学习停滞的问题

    tanh：输出梯度分布中心为0（也就是激活函数对权值变动最敏感，梯度变化最明显的位置），可缓解传导造成的学习速率降低问题，但仍然存在梯度饱和的问题

    梯度饱和的问题可以通过对训练数据执行normalization，将其分布强制拉回0-1缓解

    relu：简单的分段非线性函数，不存在梯度饱和问题，但由于输入为负数时函数值为0，可能会在学习率过高的情况下导致人工神经元坏死的问题，缓解方法是收小学习率，或换用负数段为一个斜率极小的正比例函数的leakyRELU

    Backbone 主要负责对输入图像进行特征提取。
    Neck 负责对特征图进行多尺度特征融合，并把这些特征传递给预测层。
    Head 进行最终的回归预测。

    YOLO v5的loss也由三部分组成，Classes loss和Objectness loss都使用的是BCE loss，Location loss为CIoU loss。三个预测层的Objectness loss是有不同权重的，小中大分别是[4.0, 1.0, 0.4]。